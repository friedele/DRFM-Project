{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8923ea96",
   "metadata": {},
   "source": [
    "<h1> <center><u> Adversarial Attacks on Images</u> </center></h1>\n",
    "<h3 align = 'center'> Vinutha Shivakumar</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79feb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "# libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\t\n",
    "\n",
    "# libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e328e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST is an inbuilt dataset on tensorflow\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# split the data into test and train sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# check the size of the datsets\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 60000 datapoints in train dataset and 10000 datapoints in test dataset\n",
    "# each denoting pixel information of a 28 * 28 image of a handwritten number\n",
    "\n",
    "# Normalizing the data by divinding it with 255\n",
    "# The reason is because the pixel values of the image can vary between 0-255\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20c79c",
   "metadata": {},
   "source": [
    "<h3> EDA on MNIST Data <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the x , y train and test arrays into pandas dataframe for EDA purpose\n",
    "\n",
    "# unwrapping the dataset such that each row contains the pixel information of \n",
    "# each image and storing in the dataframe\n",
    "\n",
    "# using the reshape function to convert a 3d array to 2d\n",
    "# The number of rows remain same , the 28 * 28 matrix containing pixel info is\n",
    "# unwrapped into a single row\n",
    "mnist_train = pd.DataFrame(x_train.reshape(x_train.shape[0], \n",
    "                                           x_train.shape[1] * x_train.shape[2]))\n",
    "\n",
    "# repeat the same for test data\n",
    "mnist_test = pd.DataFrame(x_test.reshape(x_test.shape[0], \n",
    "                                           x_test.shape[1] * x_test.shape[2]))\n",
    "\n",
    "# Create column names for dataframe to make it more readable\n",
    "colnames = ['Pixel'+str(i) for i in range(1,785)]\n",
    "\n",
    "mnist_train.columns = colnames\n",
    "mnist_test.columns = colnames\n",
    "\n",
    "# concatenate the labels into the test and train dataframe\n",
    "mnist_train['label'] = y_train\n",
    "mnist_test['label'] = y_test\n",
    "\n",
    "\n",
    "mnist_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5467002",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f573483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any NaN or missing datapoints\n",
    "mnist_train.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071762ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label distribution in test and train datasets\n",
    "sns.countplot(x=\"label\", data=mnist_train, palette=\"Set3\").set(\n",
    "    title=\"Label distribution in Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"label\", data=mnist_test, palette=\"Set2\").set(\n",
    "    title=\"Label distribution in Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d353d3",
   "metadata": {},
   "source": [
    "There is no imbalance in the dataset wrt the distribution of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15106241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise some of the images from train dataset\n",
    "# display 4 images per row\n",
    "fig, axes = plt.subplots(3,4, figsize=(10,5))\n",
    "# set a tight layout for better spacing between plots\n",
    "fig.tight_layout()\n",
    "axes = axes.flatten()\n",
    "# generate 12 random row numbers to select from mnist_train dataframe\n",
    "idx = np.random.randint(0,mnist_train.shape[0],size=12)\n",
    "for i in range(12):\n",
    "    # get the row data\n",
    "    pixel_data = mnist_train.iloc[idx[i]]\n",
    "    # use imshow to build the image using pixel info\n",
    "    # reshape is necessary because imshow expects a 2d array structure as input\n",
    "    axes[i].imshow(np.array(pixel_data[:784]).reshape(28,28), cmap='gray')\n",
    "    axes[i].axis('off') \n",
    "    # add the label of the image as title\n",
    "    axes[i].set_title(str(int(pixel_data[-1])), color= 'black', fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d451c3",
   "metadata": {},
   "source": [
    "<h3> Models <h/3>\n",
    "    <h4> XGBOOST </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# splitting the training data into train and validate datasets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
    "                                                  y_train, \n",
    "                                                  test_size=0.1)\n",
    "\n",
    "# All the numpy array structures need to be transformed to DMatrix structure\n",
    "# which is optimized specifically for XGBoost models\n",
    "# reshaping the x_train, x_val and x_test to a 2d matrix\n",
    "xgb_train_matrix = xb.DMatrix(x_train.reshape(x_train.shape[0], \n",
    "                                    x_train.shape[1] * x_train.shape[2]), \n",
    "                              label=y_train)\n",
    "\n",
    "xgb_val_matrix = xb.DMatrix(x_val.reshape(x_val.shape[0], \n",
    "                                    x_val.shape[1] * x_val.shape[2]), \n",
    "                              label=y_val)\n",
    "\n",
    "xgb_test_matrix = xb.DMatrix(x_test.reshape(x_test.shape[0], \n",
    "                                    x_test.shape[1] * x_test.shape[2]), \n",
    "                              label=y_test)\n",
    "\n",
    "# set the hyperparameters for the model\n",
    "params = {\n",
    "    'max_depth': 10,                # the maximum depth of each tree\n",
    "    'eta': 0.7,                     # the training step for each iteration\n",
    "    'objective': 'multi:softmax',   # multiclass classification using the softmax objective\n",
    "    'num_class': 10,                # labels range from 0-9 hence the num_classes is 10\n",
    "    'eval_metric': ['merror' ,      # evaluation metric as mean squared error\n",
    "                    'mlogloss']     # and log loss\n",
    "}  \n",
    "\n",
    "results = {}\n",
    "xgb_model = xb.train(params, xgb_train_matrix, evals=[(xgb_train_matrix, 'train'),\n",
    "                                                      (xgb_val_matrix, 'val')],\n",
    "                     num_boost_round=10,\n",
    "                     evals_result = results,\n",
    "                     verbose_eval=True)\n",
    "\n",
    "y_pred = xgb_model.predict(xgb_test_matrix)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2852b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(results['train']['merror']))\n",
    "plt.plot(epochs,results['train']['merror'], color='green', label='Train' )\n",
    "plt.plot(epochs,results['val']['merror'], color='red', label='Validation' )\n",
    "plt.title('XGBoost Classification error')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,results['train']['mlogloss'], color='blue', label='Train' )\n",
    "plt.plot(epochs,results['val']['mlogloss'], color='orange', label='Validation' )\n",
    "plt.title('XGBoost Log Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ab547",
   "metadata": {},
   "source": [
    "<h4> CNN </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical  # convert to one-hot-encoding\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose\n",
    "from keras import optimizers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channel dimension to the images\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "x_val= np.expand_dims(x_val, axis=-1)\n",
    "\n",
    "# one hot encoding all the labels\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)\n",
    "y_val = to_categorical(y_val,10)\n",
    "\n",
    "cnn_model = models.Sequential()\n",
    "\n",
    "cnn_model.add(Conv2D(filters=20, kernel_size=(5, 5), \n",
    "                   activation='relu', padding=\"same\", \n",
    "                   input_shape=(28,28,1)))\n",
    "cnn_model.add(BatchNormalization(axis=-1))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv2D(filters=20, kernel_size=(4, 4), \n",
    "                   activation='relu', padding=\"same\"))\n",
    "cnn_model.add(BatchNormalization(axis=-1))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv2D(filters=20, kernel_size=(4, 4), \n",
    "                   activation='relu', padding=\"same\"))\n",
    "cnn_model.add(BatchNormalization(axis=-1))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(200, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn_model_opt = optimizers.Adam(decay=1e-4)\n",
    "\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer = cnn_model_opt , loss = \"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"]) \n",
    "\n",
    "cnn_model_fit = cnn_model.fit(x_train, \n",
    "                        y_train,\n",
    "                        validation_data = (x_val, y_val),\n",
    "                        batch_size=128,\n",
    "                        epochs=10)\n",
    "\n",
    "cnn_test_accuracy = cnn_model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy on test data is : \", cnn_test_accuracy[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "# plot the accuracy for the train and validation datset\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cnn_model_fit.history['accuracy'], color='blue', \n",
    "         label=\"Training accuracy\")\n",
    "plt.plot(cnn_model_fit.history['val_accuracy'], color='orange', \n",
    "         label=\"Validation accuracy\")\n",
    "plt.title(\"Accuracy of CNN model\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss of train and validation dataset\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(cnn_model_fit.history['loss'], color='blue', \n",
    "         label=\"Training loss\")\n",
    "plt.plot(cnn_model_fit.history['val_loss'], color='orange', \n",
    "         label=\"Validation loss\")\n",
    "plt.title(\"Cross Entropy Loss of CNN model\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23669f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising examples that are misclassified\n",
    "y_test_pred = cnn_model.predict(x_test)\n",
    "errors = np.absolute(y_test_pred - y_test)\n",
    "errors = [np.round(np.sum(each)) for each in errors]\n",
    "\n",
    "# check the unique error values generated\n",
    "set(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28608873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since 2 is the max error difference i will fetch these indices\n",
    "# and check if the index with maxvalue match in our prediction and test label\n",
    "# fetching the max value because of one hot encoding\n",
    "error_index = [i for i, x in enumerate(errors) if (x==2.0 and \n",
    "                                                       (np.argmax(y_test_pred[i])\n",
    "                                                        !=\n",
    "                                                        np.argmax(y_test[i])))]\n",
    "\n",
    "# plot the images\n",
    "plt.figure(figsize=(20,10))\n",
    "for i,index in enumerate(error_index[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.title(\"Predicted : {}, Actual : {}\".format(np.argmax(y_test_pred[index]),\n",
    "                                                   np.argmax(y_test[index])))\n",
    "    plt.imshow(np.reshape(x_test[index],(28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the same steps as above where error difference is 1.0\n",
    "error_index = [i for i, x in enumerate(errors) if (x==1.0 and \n",
    "                                                       (np.argmax(y_test_pred[i])\n",
    "                                                        !=\n",
    "                                                        np.argmax(y_test[i])))]\n",
    "\n",
    "# plot the images\n",
    "plt.figure(figsize=(20,10))\n",
    "for i,index in enumerate(error_index[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.title(\"Predicted : {}, Actual : {}\".format(np.argmax(y_test_pred[index]),\n",
    "                                                   np.argmax(y_test[index])))\n",
    "    plt.imshow(np.reshape(x_test[index],(28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f842896",
   "metadata": {},
   "source": [
    "<h3> Adversarial Attacks </h3>\n",
    "<p> You can rerun the cells implementing the attacks multiple times to see how often they fool the network. All three attacks do not have an 100% accuracy in fooling the CNN. </p>\n",
    "<h4> FGSM </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching all the test cases which have been predicted correctly by the model\n",
    "# I will be creating adversarial examples on these image data to show how\n",
    "# different types of attack confuse the neural network and misclassify the image\n",
    "index_correctly_classified = [\n",
    "    i for i, x in enumerate(errors)\n",
    "    if (x == 0.0 and (np.argmax(y_test_pred[i]) == np.argmax(y_test[i])))\n",
    "]\n",
    "\n",
    "x_test_correctly_classified = x_test[index_correctly_classified]\n",
    "y_test_correctly_classified = y_test[index_correctly_classified]\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "def FGSM(model, image, label, eps):\n",
    "    # cast the image to float object\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # as tensorflow to start recording gradients of image\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        # use model predictions to obtain loss\n",
    "        pred = model(image)\n",
    "        loss = loss_object(label, pred)\n",
    "# calculate gradients of loss wrt the original image\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    # get the sign of gradients which will be a vector of -1,1 or 0\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    # update the original image with the signed gradients\n",
    "    # using a small epsilon will ensure that the changes to image\n",
    "    # arent visible to human eye\n",
    "    adversary = (image + (signed_grad * eps)).numpy()\n",
    "    return adversary\n",
    "\n",
    "\n",
    "# randomly select 5 images and run fgsm attacks on them\n",
    "for i in random.sample(range(0, x_test_correctly_classified.shape[0]), 5):\n",
    "    image = x_test_correctly_classified[i].reshape(1, 28, 28, 1)\n",
    "    label = y_test_correctly_classified[i].reshape(1, 10)\n",
    "    perturbed_image = FGSM(cnn_model, image, label, eps=0.1)\n",
    "    predicted_label = np.argmax(cnn_model.predict(perturbed_image))\n",
    "    actual_label = np.argmax(label)\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original image \\n Predicted : {}, Actual : {}\".format(\n",
    "        np.argmax(label), actual_label))\n",
    "    plt.imshow(np.reshape(x_test_correctly_classified[i], (28, 28)),\n",
    "               cmap=\"gray\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(perturbed_image[0] * 0.5 + 0.5, cmap=\"gray\")\n",
    "    plt.title(\"Epsilon : 0.1 \\n Predicted : {} , Actual : {}\".format(\n",
    "        predicted_label, actual_label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on xgboost for the last adversary created in the above loop\n",
    "\n",
    "xgb_pred = xgb_model.predict(xb.DMatrix(perturbed_image.reshape(1,28*28)))\n",
    "print(\"Actual label : {} , Predicted by XGboost : {}\".format(actual_label,xgb_pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45460a0d",
   "metadata": {},
   "source": [
    "<h4> Deepfool Attack </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFool(image, model):\n",
    "    # cast image to float\n",
    "    image = tf.cast(image , tf.float32)\n",
    "    adversary= image\n",
    "    x = tf.Variable(adversary)\n",
    "    fs = model(x)\n",
    "    # the model gives probabilities associated with each label\n",
    "    # we sort them in descending order and store the indexes\n",
    "    # this is useful for projecting the image onto the hyperplane of the \n",
    "    # next closest label prediction to check if it confuses the model\n",
    "    label_list = np.argsort(fs)[0][::-1]\n",
    "    actual_label = label_list[0]\n",
    "    \n",
    "    # set the initial gradient change to 0 \n",
    "    w = np.zeros(np.shape(image))\n",
    "    # perturbation to image is also set to 0\n",
    "    r_tot = np.zeros(np.shape(image))\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    # the initial label will be the same as actual label\n",
    "    k_i = actual_label\n",
    "    \n",
    "    # keep looping until there is a change in label prediction upto 50 iterations\n",
    "    while (k_i == actual_label and iteration < 50):\n",
    "        # initial perturbation is set to infinity as we want to find\n",
    "        # the minimum perturbartion to image to misclassify\n",
    "        pert = np.inf\n",
    "        # start recording gradient of image\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            fs = model(x)\n",
    "            # the loss here taken as the logits probability of the actual label\n",
    "            loss = fs[0, label_list[0]]\n",
    "        # get the gradient of loss wrt image\n",
    "        grad_orig = tape.gradient(loss, x)\n",
    "        \n",
    "        # start checking which projection gives least perturbation\n",
    "        for k in range(1, 10):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(x)\n",
    "                fs = model(x)\n",
    "                # get the logit prob wrt the label we are iteration over\n",
    "                loss = fs[0, label_list[k]]\n",
    "            # get gradient of loss wrt image over the new label\n",
    "            cur_grad = tape.gradient(loss, x)\n",
    "            \n",
    "            # check if there is any difference in gradients\n",
    "            w_k = cur_grad - grad_orig\n",
    "            \n",
    "            # change in logits probability\n",
    "            f_k = (fs[0, label_list[k]] - fs[0, label_list[0]]).numpy()\n",
    "            \n",
    "            try:\n",
    "                # np.linalg.norm without any arguments defaults to 2-norm\n",
    "                pert_k = abs(f_k) / np.linalg.norm(tf.reshape(w_k, [-1]))\n",
    "            except ZeroDivisionError:\n",
    "                # if there is near 0 change in gradient move on to next label\n",
    "                # to avoid division by zero error\n",
    "                break\n",
    "            \n",
    "            # update to lowest perturbation and gradient\n",
    "            if pert_k < pert:\n",
    "                pert = pert_k\n",
    "                w = w_k\n",
    "\n",
    "        # calculate minimal vector that projects onto the hyperplane with\n",
    "        # minimal perturbation\n",
    "        r_i = pert  * w / np.linalg.norm(w)\n",
    "        # acumulate the vector of perturbation every iteration\n",
    "        r_tot = np.float32(r_tot + r_i)\n",
    "        # add perturbation to image\n",
    "        adversary = image +r_tot\n",
    "        \n",
    "        # update the image to adversary\n",
    "        x = tf.Variable(adversary)\n",
    "        \n",
    "        # get new  predictions after adding perturbations\n",
    "        fs = model(x)\n",
    "        k_i = np.argmax(np.array(fs).flatten())\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "    return k_i, adversary   \n",
    "\n",
    "# Deepfool attack on MNIST dataset distorts the data in such a way that is even \n",
    "# visible to human eye, this attack when run on other datasets which are much bigger\n",
    "# and have a good amount of RGB data like CIFAR-10 the image distortions arent as evident \n",
    "# as the MNIST distprtions. Proof of this will be attached in the report \n",
    "for i in random.sample(range(0, x_test_correctly_classified.shape[0]),5):                \n",
    "    image = np.reshape(x_test_correctly_classified[i] , (1,28,28,1))\n",
    "    label = cnn_model(image)\t\n",
    "    actual_label = np.argmax(label)\n",
    "    label_pert, pert_image = DeepFool(image, cnn_model)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Original image \\n Predicted : {}, Actual : {}\".format(np.argmax(label),\n",
    "                                                   actual_label))\n",
    "    plt.imshow(np.reshape(x_test_correctly_classified[i],(28,28)), cmap=\"gray\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.reshape(pert_image[0],(28,28)) ,cmap=\"gray\")\n",
    "    plt.title(\"Attacked image \\n Predicted : {} , Actual : {}\".format(\n",
    "                                                            label_pert,\n",
    "                                                            actual_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on xgboost for the last adversary created in the above loop\n",
    "\n",
    "xgb_pred = xgb_model.predict(xb.DMatrix(np.reshape(pert_image,(1,28*28))))\n",
    "print(\"Actual label : {} , Predicted by XGboost : {}\".format(actual_label,xgb_pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb55626",
   "metadata": {},
   "source": [
    "<h4> LBFGS </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e80d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import contextlib\n",
    "import time\n",
    "\n",
    "# The below functions(np_value, timed_execution and run) are used to extract the \n",
    "# minimized gradients from the lbfgs optimizer\n",
    "def np_value(tensor):\n",
    "  \"\"\"Get numpy value out of possibly nested tuple of tensors.\"\"\"\n",
    "  if isinstance(tensor, tuple):\n",
    "    return type(tensor)(*(np_value(t) for t in tensor))\n",
    "  else:\n",
    "    return tensor.numpy()\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timed_execution():\n",
    "  t0 = time.time()\n",
    "  yield\n",
    "  dt = time.time() - t0\n",
    "  print('Evaluation took: %f seconds' % dt)\n",
    "  \n",
    "def run(optimizer):\n",
    "  \"\"\"Run an optimizer and measure it's evaluation time.\"\"\"\n",
    "  optimizer()  # Warmup.\n",
    "  with timed_execution():\n",
    "    result = optimizer()\n",
    "  return np_value(result)\n",
    "\n",
    "  \n",
    "def LBFGS(model, image, actual_label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # initial guess\n",
    "    # the noise will be 0\n",
    "    x0 = np.zeros(np.prod(image.shape), dtype=\"float32\").reshape(1,28,28,1)\t\n",
    "    # the function to be minimized by the lbfgs optimzer\n",
    "    # using the gradient tape to keep track of the gradients of image\n",
    "    # and obtaining the loss and associated gradient of image that has\n",
    "    # some noise added to it\n",
    "    def loss_grad(params):\n",
    "        adversary = tf.cast(image+params,tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adversary)\n",
    "            # use model predictions to obtain loss\n",
    "            pred = model(adversary)\n",
    "            loss = pred[0, actual_label]\n",
    "        # calculate gradients of loss wrt the original image\n",
    "        gradient = tf.cast(tape.gradient(loss, adversary), tf.float32)\n",
    "        return loss, gradient\n",
    "    \n",
    "    # call the optimizer function\n",
    "    def optim():\n",
    "        return tfp.optimizer.lbfgs_minimize(loss_grad,\n",
    "                                     initial_position=x0,\n",
    "                                     tolerance=1e-08)\n",
    "    # runs the lbfgs optimizers and\n",
    "    # extracts the tensors containing the minimal gradient change which\n",
    "    # will convert the image to an adversarial example\n",
    "    res = run(optim)\n",
    "    # the .position gives access to the tensor with minimum gradient value\n",
    "    # use this to add noise to original image\n",
    "    adversary = image + res.position\n",
    "    return adversary\n",
    "\n",
    "# randomly select 5 images and run fgsm attacks on them\n",
    "for i in random.sample(range(0, x_test_correctly_classified.shape[0]),5):\n",
    "    image = x_test_correctly_classified[i].reshape(1,28,28,1)\n",
    "    label = y_test_correctly_classified[i].reshape(1,10)\n",
    "    actual_label = np.argmax(label)\n",
    "    perturbed_image = LBFGS(cnn_model,image,actual_label)\n",
    "    predicted_label = np.argmax(cnn_model.predict(perturbed_image))\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Original image \\n Predicted : {}, Actual : {}\".format(np.argmax(label),\n",
    "                                                   actual_label))\n",
    "    plt.imshow(np.reshape(x_test_correctly_classified[i],(28,28)), cmap=\"gray\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(perturbed_image[0]*0.5+0.5, cmap=\"gray\")\n",
    "    plt.title(\"Perturbed image \\n Predicted : {} , Actual : {}\".format(\n",
    "                                                            predicted_label,\n",
    "                                                            actual_label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f02427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on xgboost for the last adversary created in the above loop\n",
    "\n",
    "xgb_pred = xgb_model.predict(xb.DMatrix(np.reshape(perturbed_image,(1,28*28))))\n",
    "print(\"Actual label : {} , Predicted by XGboost : {}\".format(actual_label,xgb_pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe3ec7",
   "metadata": {},
   "source": [
    "We can see that the adversarial examples are successful in fooling both the xgboost as well as CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696aabad",
   "metadata": {},
   "source": [
    "<h3> Defence against Adversarial Attack </h3>\n",
    "<h4> Adversarial Training on FGSM</h4>\n",
    "\n",
    "<p> We can observe in this technique that post adversarial training the overall accuracy of the testing data has reduced. One way to improve this is to shuffle the test data with adversarial images and then check for accuracy </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea783b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fgsm_examples(model):\n",
    "    while True:\n",
    "        adversary_images = []\n",
    "        adversary_label = []\n",
    "        # train on over 5000 adversarial images\n",
    "        index  = random.sample(range(0,x_test.shape[0]), 5000)\n",
    "        for i in index:\n",
    "            image = x_test[i].reshape(1,28,28,1)\n",
    "            label = y_test[i].reshape(1,10)\n",
    "            adversary = FGSM(model,image, label,eps=0.1)\n",
    "            # store the perturbed image into the list\n",
    "            adversary_images.append(adversary.reshape(28,28,1))\n",
    "            # we use the original label associated with it and store that\n",
    "            # into the adversary label list\n",
    "            adversary_label.append(y_test[i])\n",
    "    \n",
    "        yield (np.array(adversary_images), np.array(adversary_label))\n",
    "\n",
    "(fgsm_images, fgsm_labels) = next(create_fgsm_examples(cnn_model))\n",
    "(loss, acc) = cnn_model.evaluate(x=fgsm_images, y=fgsm_labels, verbose=0)\n",
    "\n",
    "print(\"Loss: {}, Accuracy: {}\\n\".format(loss, acc))\n",
    "\n",
    "# recompile the model and this time fit with the adversarial examples generated\n",
    "cnn_model.compile(optimizer = cnn_model_opt , loss = \"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"]) \n",
    "cnn_model.fit(fgsm_images, fgsm_labels,\n",
    "              batch_size=128,\n",
    "              epochs=10)\n",
    "\n",
    "(loss, acc) = cnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(\"Post adversarial training\")\n",
    "print(\"Loss: {}, Accuracy: {}\\n\".format(loss, acc))\n",
    "# do a final evaluation of the model on the adversarial images\n",
    "(loss, acc) = cnn_model.evaluate(x=fgsm_images, y=fgsm_labels, verbose=0)\n",
    "print(\"Evaluating on adversarial examples\")\n",
    "print(\"Loss: {}, Accuracy: {}\\n\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test = np.concatenate([x_test, fgsm_images])\n",
    "new_y_test = np.concatenate([y_test, fgsm_labels])\n",
    "\n",
    "# recompile model\n",
    "cnn_model.compile(optimizer = cnn_model_opt , loss = \"categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"]) \n",
    "cnn_model.fit(new_x_test, new_y_test,\n",
    "              batch_size=128,\n",
    "              epochs=10)\n",
    "\n",
    "(loss, acc) = cnn_model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "print(\"Post adversarial training with original test images\")\n",
    "print(\"Loss: {}, Accuracy: {}\\n\".format(loss, acc))\n",
    "# do a final evaluation of the model on the adversarial images\n",
    "(loss, acc) = cnn_model.evaluate(x=fgsm_images, y=fgsm_labels, verbose=0)\n",
    "print(\"Evaluating on adversarial examples\")\n",
    "print(\"Loss: {}, Accuracy: {}\\n\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc8cac",
   "metadata": {},
   "source": [
    "The accuracy of the model is a lot better now\n",
    "\n",
    "<h4> APE-GAN </h4>\n",
    "\n",
    "This part of the code takes more than 60 minutes(for epoch=10) to finish running. In the end we can see that the GAN model has stabilized with low loss and an accuracy of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# loading the dataset again as I need to normalize the train data to [-1,1] range\n",
    "(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "train_data = np.concatenate([x_train, fgsm_images])\n",
    "train_data = train_data.reshape(train_data.shape[0],28,28,1).astype('float32')\n",
    "train_data = (train_data - 127.5)/127.5 \n",
    "\n",
    "# create the generator model\n",
    "def generator_model():\n",
    "    gen_model = models.Sequential()\n",
    "    gen_model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(LeakyReLU())\n",
    "    \n",
    "    gen_model.add(Reshape((7, 7, 256)))\n",
    "    assert gen_model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    gen_model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert gen_model.output_shape == (None, 7, 7, 128)\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(LeakyReLU())\n",
    "    \n",
    "    gen_model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), \n",
    "                                  padding='same', use_bias=False))\n",
    "    assert gen_model.output_shape == (None, 14, 14, 64)\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(LeakyReLU())\n",
    "\n",
    "    gen_model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2),\n",
    "                                  padding='same', use_bias=False, \n",
    "                                  activation='tanh'))\n",
    "    return gen_model\n",
    "\n",
    "# create the discriminator model\n",
    "def discriminator_model():\n",
    "    disc_model = models.Sequential()\n",
    "    disc_model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    disc_model.add(LeakyReLU())\n",
    "    disc_model.add(Dropout(0.2))\n",
    "\n",
    "    disc_model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    disc_model.add(LeakyReLU())\n",
    "    disc_model.add(Dropout(0.2))\n",
    "\n",
    "    disc_model.add(Flatten())\n",
    "    disc_model.add(Dense(1))\n",
    "    return disc_model\n",
    "\n",
    "generator = generator_model()\n",
    "discriminator = discriminator_model()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=discriminator_optimizer)\n",
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 256\n",
    "BUFFER_SIZE = train_data.shape[0]\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# start builiding the GAN model\n",
    "# freeze the discrimintaor weights\n",
    "discriminator.trainable = False\n",
    "\n",
    "test_input = Input(shape=(noise_dim,))\n",
    "# combine the generator and discriminator model\n",
    "test_output = discriminator(generator(test_input))\n",
    "\n",
    "GAN_model = Model(test_input, test_output)\n",
    "# use the Adam optimizer for GAN\n",
    "GAN_model.compile(loss=\"binary_crossentropy\", optimizer=discriminator_optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "noise = tf.random.normal([256, noise_dim])\n",
    "\n",
    "# create batch data on the training dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch {} of {}\".format(epoch+1, EPOCHS))\n",
    "    for image_batch in train_data:\n",
    "        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "        gen_img = generator.predict(noise, verbose=0)\n",
    "        x = np.concatenate([image_batch, gen_img])\n",
    "        # creating labels as 1 and 0 for discriminator as it has to just classify as real or fake\n",
    "        y = np.reshape(([1] *int(x.shape[0]/2)) + ([0] * int(x.shape[0]/2)),(-1,))\n",
    "        # train discrimnator on both original images and with noise\n",
    "        discriminator_loss = discriminator.train_on_batch(x,y)        \n",
    "        perturbed_labels = np.reshape([1]* BATCH_SIZE,(-1,))\n",
    "        # use the noise to and labels to train GAN model\n",
    "        GAN_loss = GAN_model.train_on_batch(noise, perturbed_labels)        \n",
    "    \n",
    "    print(\"Discriminator Loss={}, GAN loss={} \".format(discriminator_loss, GAN_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
