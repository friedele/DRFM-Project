{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6881b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, qr, eig, norm\n",
    "import math\n",
    "from math import isclose, sqrt\n",
    "#from tqdm import tqdm\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--storage', '-m', default=10, help='The Memory Storage')\n",
    "parser.add_argument('--mini_batch','-minibatch', default=1000,help='minibatch size')\n",
    "parser.add_argument('--num_batch_in_data', '-num-batch',default=5,\n",
    "        \t\t\t\t\t\t\thelp='number of batches with overlap')\n",
    "parser.add_argument('--method', '-method',default='trust-region',\n",
    "        \thelp=\"\"\"Method of optimization ['line-search','trust-region']\"\"\")\n",
    "parser.add_argument(\n",
    "        '--whole_gradient','-use-whole-data', action='store_true',default=False,\n",
    "        help='Compute the gradient using all data')\n",
    "parser.add_argument('--max_iter', '-maxiter', default=200, help='max iterations')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "minibatch = int(args.mini_batch)\n",
    "m = int(args.storage)\n",
    "num_batch_in_data = int(args.num_batch_in_data)\n",
    "use_whole_data = args.whole_gradient\n",
    "# if minibatch==500: ==> num_batch_in_data in [3, 6, 9, 12, 18, 36, 54, 108]\n",
    "# if minibatch==1000 ==> num_batch_in_data in [3, 6, 9, 18, 54]\n",
    "# if minibatch ==540 ==> num_batch_in_data in [5, 10, 20, 25, 50, 100]\n",
    "# if minibatch ==1080 ==> num_batch_in_data in [5, 10, 25, 50]\n",
    "method = str(args.method)\n",
    "# ['line-search','trust-region']\n",
    "max_num_iter = int(args.max_iter)\n",
    "\n",
    "iter_num = 0\n",
    "###############################################################################\n",
    "######################## MNIST DATA ###########################################\n",
    "###############################################################################\n",
    "import input_MNIST_data\n",
    "from input_MNIST_data import shuffle_data\n",
    "data = input_MNIST_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "X_train, y_train = shuffle_data(data)\n",
    "# input and output shape\n",
    "n_input   = data.train.images.shape[1]  # here MNIST data input (28,28)\n",
    "n_classes = data.train.labels.shape[1]  # here MNIST (0-9 digits)\n",
    "\n",
    "X_test = data.test.images\n",
    "y_test = data.test.labels\n",
    "\n",
    "X_validation = data.validation.images\n",
    "y_validation = data.validation.labels\n",
    "\n",
    "X_train_multi = []\n",
    "y_train_multi = []\n",
    "###############################################################################\n",
    "######################## LBFGS PARAMS #########################################\n",
    "###############################################################################\n",
    "\n",
    "S = np.array([[]])\n",
    "Y = np.array([[]])\n",
    "gamma = 1\n",
    "lambda_min = 0\n",
    "alpha = 1\n",
    "# GLOBAL VARIABLES - MATRICES\n",
    "P_ll = np.array([[]]) # P_parallel \n",
    "g_ll = np.array([[]]) # g_Parallel\n",
    "g_NL_norm = 0\n",
    "Lambda_1 = np.array([[]])\n",
    "\n",
    "g = np.array([])\n",
    "###############################################################################\n",
    "######################## LeNet-5 Network Architecture #########################\n",
    "###############################################################################\n",
    "# number of weights and bias in each layer\n",
    "n_W = {}\n",
    "dim_w = {}\n",
    "\n",
    "# network architecture hyper parameters\n",
    "input_shape = [-1,28,28,1]\n",
    "W0 = 28\n",
    "H0 = 28\n",
    "\n",
    "# Layer 1 -- conv\n",
    "D1 = 1; F1 = 5; K1 = 20; S1 = 1\n",
    "W1 = (W0 - F1) // S1 + 1\n",
    "H1 = (H0 - F1) // S1 + 1\n",
    "conv1_dim = [F1, F1, D1, K1]\n",
    "conv1_strides = [1,S1,S1,1] \n",
    "n_W['1_w_conv'] = F1 * F1 * D1 * K1\n",
    "n_W['1_b_conv'] = K1 \n",
    "dim_w['1_w_conv'] = [F1, F1, D1, K1]\n",
    "dim_w['1_b_conv'] = [K1]\n",
    "\n",
    "# Layer 2 -- max pool\n",
    "D2 = K1; F2 = 2; K2 = D2; S2 = 2\n",
    "W2 = (W1 - F2) // S2 + 1\n",
    "H2 = (H1 - F2) // S2 + 1\n",
    "layer2_ksize = [1,F2,F2,1]\n",
    "layer2_strides = [1,S2,S2,1]\n",
    "\n",
    "# Layer 3 -- conv\n",
    "D3 = K2; F3 = 5; K3 = 50; S3 = 1\n",
    "W3 = (W2 - F3) // S3 + 1\n",
    "H3 = (H2 - F3) // S3 + 1\n",
    "conv2_dim = [F3, F3, D3, K3]\n",
    "conv2_strides = [1,S3,S3,1] \n",
    "n_W['2_w_conv'] = F3 * F3 * D3 * K3\n",
    "n_W['2_b_conv'] = K3 \n",
    "dim_w['2_w_conv'] = [F3, F3, D3, K3]\n",
    "dim_w['2_b_conv'] = [K3]\n",
    "\n",
    "# Layer 4 -- max pool\n",
    "D4 = K3; F4 = 2; K4 = D4; S4 = 2\n",
    "W4 = (W3 - F4) // S4 + 1\n",
    "H4 = (H3 - F4) // S4 + 1\n",
    "layer4_ksize = [1,F4,F4,1]\n",
    "layer4_strides = [1,S4,S4,1]\n",
    "\n",
    "\n",
    "# Layer 5 -- fully connected\n",
    "n_in_fc = W4 * H4 * D4\n",
    "n_hidden = 500\n",
    "fc_dim = [n_in_fc,n_hidden]\n",
    "n_W['3_w_fc'] = n_in_fc * n_hidden\n",
    "n_W['3_b_fc'] = n_hidden\n",
    "dim_w['3_w_fc'] = [n_in_fc,n_hidden]\n",
    "dim_w['3_b_fc'] = [n_hidden]\n",
    "# Layer 6 -- output\n",
    "n_in_out = n_hidden\n",
    "n_W['4_w_fc'] = n_hidden * n_classes\n",
    "n_W['4_b_fc'] = n_classes\n",
    "dim_w['4_w_fc'] = [n_hidden,n_classes]\n",
    "dim_w['4_b_fc'] = [n_classes]\n",
    "\n",
    "\n",
    "for key, value in n_W.items():\n",
    "\tn_W[key] = int(value)\n",
    "\n",
    "###############################################################################\n",
    "######################## f(x;w) ###############################################\n",
    "###############################################################################\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "w_initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "w_tf = {}\n",
    "for key, _ in dim_w.items():\n",
    "\tw_tf[key] = tf.get_variable(key, shape=dim_w[key], \n",
    "\t\t\t\t\t\t\t\t\t\t\tinitializer=w_initializer)\n",
    "\n",
    "def lenet5_model(x,_w):\n",
    "\t# Reshape input to a 4D tensor \n",
    "\tx = tf.reshape(x, shape = input_shape)\n",
    "\t# LAYER 1 -- Convolution Layer\n",
    "\tconv1 = tf.nn.relu(tf.nn.conv2d(input = x, \n",
    "\t\t\t\t\t\t\t\t\tfilter =_w['1_w_conv'],\n",
    "\t\t\t\t\t\t\t\t\tstrides = [1,S1,S1,1],\n",
    "\t\t\t\t\t\t\t\t\tpadding = 'VALID') + _w['1_b_conv'])\n",
    "\t# Layer 2 -- max pool\n",
    "\tconv1 = tf.nn.max_pool(\tvalue = conv1, \n",
    "\t\t\t\t\t\t\tksize = [1, F2, F2, 1], \n",
    "\t\t\t\t\t\t\tstrides = [1, S2, S2, 1], \n",
    "\t\t\t\t\t\t\tpadding = 'VALID')\n",
    "\n",
    "\t# LAYER 3 -- Convolution Layer\n",
    "\tconv2 = tf.nn.relu(tf.nn.conv2d(input = conv1, \n",
    "\t\t\t\t\t\t\t\t\tfilter =_w['2_w_conv'],\n",
    "\t\t\t\t\t\t\t\t\tstrides = [1,S3,S3,1],\n",
    "\t\t\t\t\t\t\t\t\tpadding = 'VALID') + _w['2_b_conv'])\n",
    "\t# Layer 4 -- max pool\n",
    "\tconv2 = tf.nn.max_pool(\tvalue = conv2 , \n",
    "\t\t\t\t\t\t\tksize = [1, F4, F4, 1], \n",
    "\t\t\t\t\t\t\tstrides = [1, S4, S4, 1], \n",
    "\t\t\t\t\t\t\tpadding = 'VALID')\n",
    "\t# Fully connected layer\n",
    "\t# Reshape conv2 output to fit fully connected layer\n",
    "\tfc = tf.contrib.layers.flatten(conv2)\n",
    "\tfc = tf.nn.relu(tf.matmul(fc, _w['3_w_fc']) + _w['3_b_fc'])\n",
    "\t# fc = tf.nn.dropout(fc, dropout_rate)\n",
    "\n",
    "\ty_ = tf.matmul(fc, _w['4_w_fc']) + _w['4_b_fc']\n",
    "\treturn y_\n",
    "\n",
    "# Construct model\n",
    "model = lenet5_model\n",
    "y_ = model(x,w_tf)\n",
    "\n",
    "# Softmax loss\n",
    "loss = tf.reduce_mean(\n",
    "\ttf.nn.softmax_cross_entropy_with_logits(labels = y, logits = y_))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "###############################################################################\n",
    "######################## TF GRADINETS #########################################\n",
    "###############################################################################\n",
    "grad_w_tf = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\tgrad_w_tf[layer] = tf.gradients(xs=w_tf[layer], ys=loss)\n",
    "\n",
    "###############################################################################\n",
    "######################## TF Auxilary variables ################################\n",
    "###############################################################################\n",
    "aux_w = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\tname = layer + 'aux_w_'\n",
    "\taux_w[layer] = tf.get_variable(name=name, \n",
    "\t\t\t\t\tshape=w_tf[layer].get_shape(), initializer=w_initializer)\n",
    "\n",
    "aux_w_placeholder = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\taux_w_placeholder[layer] = tf.placeholder(dtype=\"float\",\n",
    "\t\t\t\t\t\t\t\t\t\tshape=w_tf[layer].get_shape())\n",
    "aux_w_init = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\taux_w_init[layer] = aux_w[layer].assign(aux_w_placeholder[layer])\n",
    "\n",
    "aux_output = model(x,aux_w)\n",
    "aux_loss = tf.reduce_mean(\n",
    "\ttf.nn.softmax_cross_entropy_with_logits(labels = y, logits = aux_output))\n",
    "aux_grad_w = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\taux_grad_w[layer] = tf.gradients(xs=aux_w[layer], ys=aux_loss)\n",
    "\n",
    "update_w = {}\n",
    "update_w_placeholder = {}\n",
    "for layer, _ in w_tf.items():\n",
    "\tupdate_w_placeholder[layer] = tf.placeholder(dtype=\"float\",\n",
    "\t\t\t\t\t\t\t\t\t\tshape=w_tf[layer].get_shape())\n",
    "for layer, _ in w_tf.items():\n",
    "\tupdate_w[layer] = w_tf[layer].assign(update_w_placeholder[layer])\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "def backtracking_line_search(sess,g):\n",
    "\talpha = 1\n",
    "\trho_ls = 0.9\n",
    "\tc1 = 1E-4\n",
    "\tBLS_COND = False\n",
    "\tp = -g\n",
    "\twhile not BLS_COND:\n",
    "\t\tnew_f = eval_aux_loss(sess,alpha*p)\n",
    "\t\told_f = eval_loss(sess)\n",
    "\t\tlhs = new_f\n",
    "\t\trhs = old_f + c1 * alpha * p @ g\n",
    "\t\tBLS_COND = lhs <= rhs\n",
    "\t\t\n",
    "\t\tif BLS_COND:\n",
    "\t\t\tprint('Backtracking line search satisfied for alpha = {0:.4f}' \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.format(alpha))\n",
    "\t\tif alpha < 0.1:\n",
    "\t\t\tprint('WARNING! Backtracking line search did not work')\n",
    "\t\t\tbreak\n",
    "\t\talpha = alpha * rho_ls\n",
    "\treturn alpha*p\n",
    "\n",
    "def quad_model():\n",
    "\tpass\n",
    "\n",
    "def phi_bar_func(sigma,delta):\n",
    "\t# phi(sigma) = 1 / v(sigma) - 1 / delta\t\n",
    "\tu = sum( (g_ll ** 2) / ((Lambda_1 + sigma) ** 2) ) + \\\n",
    "\t\t\t\t\t\t\t\t\t(g_NL_norm ** 2) / ( (gamma + sigma) ** 2)\n",
    "\tv = sqrt(u) \n",
    "\n",
    "\tphi = 1 / v - 1 / delta\n",
    "\treturn phi\n",
    "\n",
    "def phi_bar_prime_func(sigma):\n",
    "\tu = sum( g_ll ** 2 / (Lambda_1 + sigma) ** 2 ) + \\\n",
    "\t\t\t\t\t\t\t\t\t\tg_NL_norm ** 2 / (gamma + sigma) ** 2\n",
    "\n",
    "\tu_prime = sum( g_ll ** 2 / (Lambda_1 + sigma) ** 3 ) + \\\n",
    "\t\t\t\t\t\t\t\t\t\tg_NL_norm ** 2 / (gamma + sigma) ** 3\n",
    "\tphi_bar_prime = u ** (-3/2) * u_prime\n",
    "\n",
    "\treturn phi_bar_prime\n",
    "\n",
    "\n",
    "def solve_newton_equation_to_find_sigma(delta):\n",
    "\t# tolerance\n",
    "\ttol = 1E-4\n",
    "\tsigma = max( 0, -lambda_min )\n",
    "\tif phi_bar_func(sigma,delta) < 0:\n",
    "\t\tsigma_hat = max( abs( g_ll ) / delta - Lambda_1 )\n",
    "\t\tsigma = max( 0, sigma_hat)\n",
    "\t\twhile( abs( phi_bar_func(sigma,delta) ) > tol ):\n",
    "\t\t\tphi_bar = phi_bar_func(sigma,delta)\n",
    "\t\t\tphi_bar_prime = phi_bar_prime_func(sigma)\n",
    "\t\t\tsigma = sigma - phi_bar / phi_bar_prime\n",
    "\t\tsigma_star = sigma\n",
    "\telif lambda_min < 0:\n",
    "\t\tsigma_star = - lambda_min\n",
    "\telse:\n",
    "\t\tsigma_star = 0\n",
    "\n",
    "\treturn sigma_star \n",
    "\n",
    "def lbfgs_line_search_subproblem_solver(sess, g):\n",
    "\t# dimension of w\n",
    "\tn = sum(n_W.values())\n",
    "\n",
    "\tPsi = np.concatenate( (gamma*S, Y) ,axis=1)\n",
    "\t\n",
    "\tS_T_Y = S.T @ Y\n",
    "\tL = np.tril(S_T_Y,k=-1)\n",
    "\tU = np.tril(S_T_Y.T,k=-1).T\n",
    "\tD = np.diag( np.diag(S_T_Y) )\n",
    "\n",
    "\tM = - inv( np.block([ \t[gamma * S.T @ S ,\tL],\n",
    "\t\t\t\t\t\t\t[     L.T,\t\t   -D] \n",
    "\t\t\t]) )\n",
    "\n",
    "\tQ, R = qr(Psi, mode='reduced')\n",
    "\teigen_values, eigen_vectors = eig( R @ M @ R.T )\n",
    "\n",
    "\t# sorted eigen values\n",
    "\tidx = eigen_values.argsort()\n",
    "\teigen_values_sorted = eigen_values[idx]\n",
    "\teigen_vectors_sorted = eigen_vectors[:,idx]\n",
    "\n",
    "\tLambda_hat = eigen_values_sorted\n",
    "\tV = eigen_vectors_sorted\n",
    "\n",
    "\tglobal P_ll\n",
    "\tglobal g_ll\n",
    "\tglobal g_NL_norm\n",
    "\tglobal Lambda_1\n",
    "\n",
    "\tLambda_1 = gamma + Lambda_hat\n",
    "\t#Lambda_2 = gamma * np.ones( n-len(Lambda_hat) )\n",
    "\t#B_diag = np.concatenate( (Lambda_1, Lambda_2),axis=0 )\n",
    "\tglobal lambda_min\n",
    "\tlambda_min = min( Lambda_1.min(), gamma )\n",
    "\t\n",
    "\tP_ll = Psi @ inv(R) @ V # P_parallel \n",
    "\tg_ll = P_ll.T @ g\t# g_Parallel\n",
    "\tg_NL_norm = sqrt ( abs( norm(g) ** 2 - norm(g_ll) ** 2 ) )\n",
    "\n",
    "\tp = - 1 / gamma * \\\n",
    "\t\t\t( g - Psi @ inv( gamma * inv(M) + Psi.T @ Psi ) @ (Psi.T @ g) )\n",
    "\n",
    "\talpha = satisfy_wolfe_condition(sess,p)\n",
    "\n",
    "\treturn alpha * p\n",
    "\n",
    "\n",
    "def satisfy_wolfe_condition(sess, p):\n",
    "\talpha = 1\n",
    "\trho_ls = 0.9\n",
    "\tc1 = 1E-4\n",
    "\tc2 = 0.9\n",
    "\tWOLFE_COND_1 = False\n",
    "\tWOLFE_COND_2 = False\n",
    "\twhile not ( WOLFE_COND_1 and WOLFE_COND_2): \n",
    "\t\tnew_f = eval_aux_loss(sess,alpha*p)\n",
    "\t\told_f = eval_loss(sess)\n",
    "\t\tlhs = new_f\n",
    "\t\trhs = old_f + c1 * alpha * p @ g\n",
    "\t\tWOLFE_COND_1 = lhs <= rhs\n",
    "\t\tif WOLFE_COND_1:\n",
    "\t\t\tprint('WOLFE_COND_1 SATISFIED')\n",
    "\t\telse:\n",
    "\t\t\tprint('WOLFE_COND_1 NOT SATISFIED')\n",
    "\n",
    "\t\tnew_g = eval_aux_gradient_vec(sess)\n",
    "\t\tlhs = new_g @ p\n",
    "\t\trhs = c2 * g @ p\n",
    "\t\tWOLFE_COND_2 = lhs >= rhs\n",
    "\t\tif WOLFE_COND_2:\n",
    "\t\t\tprint('WOLFE_COND_2 SATISFIED')\n",
    "\t\telse:\n",
    "\t\t\tprint('WOLFE_COND_2 NOT SATISFIED')\n",
    "\n",
    "\t\tif WOLFE_COND_1 and WOLFE_COND_2:\n",
    "\t\t\tprint('WOLFE CONDITIONS SATISFIED')\n",
    "\t\t\tprint('alpha = {0:.4f}' .format(alpha))\n",
    "\t\t\n",
    "\t\tif alpha < 0.1:\n",
    "\t\t\tprint('WARNING! Wolfe Condition did not satisfy')\n",
    "\t\t\tbreak\n",
    "\t\talpha = alpha * rho_ls\n",
    "\treturn alpha\n",
    "\n",
    "\n",
    "def lbfgs_trust_region_subproblem_solver(delta, g):\n",
    "\t# size of w = g.size\n",
    "\tn = sum(n_W.values())\n",
    "\n",
    "\tPsi = np.concatenate( (gamma*S, Y) ,axis=1)\n",
    "\t\n",
    "\tS_T_Y = S.T @ Y\n",
    "\tL = np.tril(S_T_Y,k=-1)\n",
    "\tU = np.tril(S_T_Y.T,k=-1).T\n",
    "\tD = np.diag( np.diag(S_T_Y) )\n",
    "\n",
    "\tM = - inv( np.block([ \t[gamma * S.T @ S ,\tL],\n",
    "\t\t\t\t\t\t\t[     L.T,\t\t   -D] \n",
    "\t\t\t]) )\n",
    "\n",
    "\tQ, R = qr(Psi, mode='reduced')\n",
    "\teigen_values, eigen_vectors = eig( R @ M @ R.T )\n",
    "\n",
    "\t# sorted eigen values\n",
    "\tidx = eigen_values.argsort()\n",
    "\teigen_values_sorted = eigen_values[idx]\n",
    "\teigen_vectors_sorted = eigen_vectors[:,idx]\n",
    "\n",
    "\tLambda_hat = eigen_values_sorted\n",
    "\tV = eigen_vectors_sorted\n",
    "\n",
    "\tglobal P_ll\n",
    "\tglobal g_ll\n",
    "\tglobal g_NL_norm\n",
    "\tglobal Lambda_1\n",
    "\n",
    "\tLambda_1 = gamma + Lambda_hat\n",
    "\t#Lambda_2 = gamma * np.ones( n-len(Lambda_hat) )\n",
    "\t#B_diag = np.concatenate( (Lambda_1, Lambda_2),axis=0 )\n",
    "\n",
    "\n",
    "\tP_ll = Psi @ inv(R) @ V # P_parallel \n",
    "\tg_ll = P_ll.T @ g\t# g_Parallel\n",
    "\tg_NL_norm = sqrt ( abs( norm(g) ** 2 - norm(g_ll) ** 2 ) )\n",
    "\n",
    "\tsigma = 0\n",
    "\tphi = phi_bar_func(sigma,delta)\n",
    "\n",
    "\tif phi >= 0:\n",
    "\t\tsigma_star = 0\n",
    "\t\ttau_star = gamma\n",
    "\telse:\n",
    "\t\tsigma_star = solve_newton_equation_to_find_sigma(delta)\n",
    "\t\ttau_star = gamma + sigma_star\n",
    "\n",
    "\tp_star = - 1 / tau_star * \\\n",
    "\t\t\t( g - Psi @ inv( tau_star * inv(M) + Psi.T @ Psi ) @ (Psi.T @ g) )\n",
    "\n",
    "\treturn p_star\n",
    "\n",
    "def eval_reduction_ratio(sess,g,p):\n",
    "\tnew_f = eval_aux_loss(sess,p)\n",
    "\told_f = eval_loss(sess)\n",
    "\n",
    "\tared = old_f - new_f\n",
    "\n",
    "\tif S.size is not 0:\n",
    "\t\tp_ll = P_ll.T @ p\n",
    "\t\tp_NL_norm = sqrt ( abs( norm(p) ** 2 - norm(p_ll) ** 2 ) )\n",
    "\t\tp_T_B_p = sum( Lambda_1 * p_ll ** 2)  + gamma * p_NL_norm ** 2\n",
    "\t\tpred =  - (g @ p  + 1/2 * p_T_B_p)\n",
    "\telse:\n",
    "\t\tpred =  - 1/2 * g @ p\n",
    "\t\n",
    "\trho = ared / pred\n",
    "\t\n",
    "\treturn rho\n",
    "\n",
    "def eval_y(sess):\n",
    "\tnew_g = eval_aux_gradient_vec(sess)\n",
    "\told_g = g\n",
    "\tnew_y = new_g - old_g\n",
    "\treturn new_y\n",
    "\n",
    "def enqueue(Z,new_val):\n",
    "\tif Z.size == 0:\n",
    "\t\tZ = new_val.reshape(-1,1)\n",
    "\t\treturn Z\n",
    "\tZ = np.concatenate( (Z,new_val.reshape(-1,1)), axis=1)\n",
    "\treturn Z\n",
    "\t\t\n",
    "def dequeue(Z):\n",
    "\treturn np.delete(Z, obj=0, axis=1)\n",
    "\n",
    "def update_S_Y(new_s_val,new_y_val):\n",
    "\tglobal S\n",
    "\tglobal Y\n",
    "\n",
    "\tStmp = S\n",
    "\tYtmp = Y\n",
    "\n",
    "\tnum_columns_S = Stmp.shape[1]\n",
    "\tnum_columns_Y = Stmp.shape[1]\n",
    "\tassert num_columns_S is num_columns_Y, \"dimention of S and Y doesn't match\"\n",
    "\tif num_columns_S < m:\n",
    "\t\tStmp = enqueue(Stmp,new_s_val)\n",
    "\t\tYtmp = enqueue(Ytmp,new_y_val)\n",
    "\telse:\n",
    "\t\tStmp = dequeue(Stmp)\n",
    "\t\tStmp = enqueue(Stmp,new_s_val)\n",
    "\t\tYtmp = dequeue(Ytmp)\n",
    "\t\tYtmp = enqueue(Ytmp,new_y_val)\n",
    "\n",
    "\tS = Stmp\n",
    "\tY = Ytmp\n",
    "\treturn \n",
    "\n",
    "\n",
    "def dict_of_weight_matrices_to_single_linear_vec(x_dict):\n",
    "\tx_vec = np.array([])\n",
    "\tfor key in sorted(w_tf.keys()):\n",
    "\t\tmatrix = x_dict[key]\n",
    "\t\tx_vec = np.append(x_vec,matrix.flatten())\t\n",
    "\treturn x_vec\n",
    "\n",
    "def linear_vec_to_dict_of_weight_matrices(x_vec):\n",
    "\tx_dict = {}\n",
    "\tid_start = 0\n",
    "\tid_end   = 0\n",
    "\tfor key in sorted(w_tf.keys()):\n",
    "\t\tid_end = id_start + n_W[key]\n",
    "\t\tvector = x_vec[id_start:id_end]\n",
    "\t\tmatrix = vector.reshape(dim_w[key])\n",
    "\t\tx_dict[key] = matrix\n",
    "\t\tid_start = id_end\n",
    "\treturn x_dict\n",
    "\n",
    "def compute_multibatch_tensor(sess,tensor_tf,X__,y__):\n",
    "\tfeed_dict = {}\n",
    "\ttotal = 0\n",
    "\tnum_minibatches_here = X__.shape[0] // minibatch\n",
    "\tfor j in range(num_minibatches_here):\n",
    "\t\tindex_minibatch = j % num_minibatches_here\n",
    "\t\t# mini batch \n",
    "\t\tstart_index = index_minibatch     * minibatch\n",
    "\t\tend_index   = (index_minibatch+1) * minibatch\n",
    "\t\tX_batch = X__[start_index:end_index]\n",
    "\t\ty_batch = y__[start_index:end_index]\n",
    "\t\tfeed_dict.update({\tx: X_batch,\n",
    "\t\t\t\t\t\t\ty: y_batch})\n",
    "\n",
    "\t\tvalue = sess.run(tensor_tf, feed_dict=feed_dict)\n",
    "\t\ttotal = total + value\n",
    "\n",
    "\ttotal = total * 1 / num_minibatches_here\t\n",
    "\treturn total\n",
    "\n",
    "def compute_multibatch_gradient(sess,grad_tf,train_set,labels):\n",
    "\tfeed_dict = {}\n",
    "\tgw = {}\n",
    "\tnum_minibatches_here = train_set.shape[0] // minibatch\n",
    "\tfor j in range(num_minibatches_here):\n",
    "\t\tindex_minibatch = j % num_minibatches_here\n",
    "\t\t# mini batch \n",
    "\t\tstart_index = index_minibatch     * minibatch\n",
    "\t\tend_index   = (index_minibatch+1) * minibatch\n",
    "\t\tX_batch = train_set[start_index:end_index]\n",
    "\t\ty_batch = labels[start_index:end_index]\n",
    "\t\tfeed_dict.update({\tx: X_batch,\n",
    "\t\t\t\t\t\t\ty: y_batch})\n",
    "\n",
    "\t\tgw_list = sess.run(grad_tf, feed_dict=feed_dict)\n",
    "\t\tif j == 0:\t\t\n",
    "\t\t\tfor layer, _ in w_tf.items():\n",
    "\t\t\t\tgw[layer] = gw_list[layer][0]\n",
    "\t\telse:\n",
    "\t\t\tfor layer, _ in w_tf.items():\n",
    "\t\t\t\tgw[layer] = gw[layer] + gw_list[layer][0]\n",
    "\n",
    "\tfor layer, _ in w_tf.items():\n",
    "\t\tgw[layer] = gw[layer] * 1 / num_minibatches_here\t\n",
    "\treturn gw\n",
    "\n",
    "def eval_gradient_vec(sess):\n",
    "\t\"\"\"returns gradient, here only for mode='robust-multi-batch' \n",
    "\tI should modify to consider all other cases\"\"\"\n",
    "\tg_dict = compute_multibatch_gradient(sess,grad_w_tf,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tX_train_multi,y_train_multi)\n",
    "\tg_vec = dict_of_weight_matrices_to_single_linear_vec(g_dict)\n",
    "\treturn g_vec\t\n",
    "\n",
    "def eval_accuracy(sess):\n",
    "\taccuracy_val = compute_multibatch_tensor(sess,accuracy,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tX_train_multi,y_train_multi)\n",
    "\treturn accuracy_val\n",
    "\n",
    "def eval_accuracy_test(sess):\n",
    "\taccuracy_val = compute_multibatch_tensor(sess,accuracy, X_test, y_test)\n",
    "\treturn accuracy_val\n",
    "\n",
    "\n",
    "def eval_accuracy_validation(sess):\n",
    "\taccuracy_val = compute_multibatch_tensor(sess,accuracy, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tX_validation,y_validation)\n",
    "\treturn accuracy_val\n",
    "\n",
    "\n",
    "def eval_w_dict(sess):\n",
    "\tw_dict = sess.run(w_tf)\n",
    "\treturn w_dict\n",
    "\n",
    "def update_weights(sess,p_vec):\n",
    "\tw_dict = eval_w_dict(sess)\n",
    "\tp_dict = linear_vec_to_dict_of_weight_matrices(p_vec)\n",
    "\tfeed_dict = {}\n",
    "\tfor key,_ in w_tf.items():\n",
    "\t\tfeed_dict.update({update_w_placeholder[key]: w_dict[key]+p_dict[key] })\n",
    "\tsess.run(update_w, feed_dict=feed_dict)\n",
    "\treturn\n",
    "\n",
    "def eval_aux_loss(sess,p_vec):\n",
    "\tw_dict = eval_w_dict(sess)\n",
    "\tp_dict = linear_vec_to_dict_of_weight_matrices(p_vec)\n",
    "\tfeed_dict = {}\n",
    "\tfor key,_ in w_tf.items():\n",
    "\t\tfeed_dict.update({aux_w_placeholder[key]: w_dict[key]+p_dict[key] })\n",
    "\tsess.run(aux_w_init,feed_dict=feed_dict)\n",
    "\tloss_new = compute_multibatch_tensor(sess,aux_loss,\n",
    "\t\t\t\t\t\t\t\t\t\t\tX_train_multi,y_train_multi)\n",
    "\treturn loss_new\n",
    "\n",
    "def eval_loss(sess):\n",
    "\tloss_val = compute_multibatch_tensor(sess,loss,X_train_multi,y_train_multi)\n",
    "\treturn loss_val\n",
    "\n",
    "def eval_loss_test(sess):\n",
    "\tloss_val = compute_multibatch_tensor(sess,loss,X_test,y_test)\n",
    "\treturn loss_val\n",
    "\n",
    "def eval_loss_validation(sess):\n",
    "\tloss_val = compute_multibatch_tensor(sess,loss,X_validation,y_validation)\n",
    "\treturn loss_val\n",
    "\n",
    "def eval_aux_gradient_vec(sess):\n",
    "\t# assuming that eval_aux_loss is being called before this function call\n",
    "\taux_g_dict = compute_multibatch_gradient(sess,aux_grad_w,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tX_train_multi,y_train_multi)\n",
    "\taux_g_vec = dict_of_weight_matrices_to_single_linear_vec(aux_g_dict)\n",
    "\treturn aux_g_vec\t\n",
    "\n",
    "###############################################################################\n",
    "######################## TRUST REGION ALGORITHM ###############################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# save training results\n",
    "loss_train_results = []\n",
    "loss_validation_results = []\n",
    "loss_test_results = []\n",
    "accuracy_train_results = []\n",
    "accuracy_validation_results = []\n",
    "accuracy_test_results = []\n",
    "def save_print_training_results(sess):\n",
    "\tloss_train = eval_loss(sess)\n",
    "\taccuracy_train = eval_accuracy(sess)\n",
    "\tloss_validation = eval_loss_validation(sess)\n",
    "\taccuracy_validation = eval_accuracy_validation(sess)\n",
    "\tloss_test = eval_loss_test(sess)\n",
    "\taccuracy_test = eval_accuracy_test(sess)\n",
    "\n",
    "\t# saving training results\n",
    "\tloss_train_results.append(loss_train)\n",
    "\tloss_validation_results.append(loss_validation)\n",
    "\tloss_test_results.append(loss_test)\n",
    "\taccuracy_train_results.append(accuracy_train)\n",
    "\taccuracy_validation_results.append(accuracy_validation)\n",
    "\taccuracy_test_results.append(accuracy_test)\n",
    "\n",
    "\tprint('LOSS     - train: {0:.4f}, validation: {1:.4f}, test: {2:.4f}' \\\n",
    "\t\t\t\t\t\t.format(loss_train, loss_validation, loss_test))\n",
    "\tprint('ACCURACY - train: {0:.4f}, validation: {1:.4f}, test: {2:.4f}' \\\n",
    "\t\t\t.format(accuracy_train, accuracy_validation, accuracy_test))\n",
    "\n",
    "def permutation(n,k):\n",
    "\tset_1 = (k%n, k%n+1)\n",
    "\tset_2 = (k%n+1, k%n+2)\n",
    "\tif k%n == n-1:\n",
    "\t\tset_2 = (0, 1)\n",
    "\treturn set_1, set_2\n",
    "\n",
    "def set_multi_batch(num_batch_in_data, iteration):\n",
    "\t\"\"\"multi batches with half size overlap\"\"\"  \n",
    "\n",
    "\tglobal X_train_multi\n",
    "\tglobal y_train_multi\n",
    "\n",
    "\tif use_whole_data:\n",
    "\t\tX_train_multi = X_train\n",
    "\t\ty_train_multi = y_train\n",
    "\t\treturn\n",
    "\n",
    "\tset_1, set_2 = permutation(num_batch_in_data,iteration)\n",
    "\toverlap_batch_size = X_train.shape[0] // num_batch_in_data\n",
    "\tstart_index_1 = set_1[0] * overlap_batch_size\n",
    "\tend_index_1 = set_1[1] * overlap_batch_size\n",
    "\tstart_index_2 = set_2[0] * overlap_batch_size\n",
    "\tend_index_2 = set_2[1] * overlap_batch_size\n",
    "\n",
    "\tX_half_batch_1 = X_train[start_index_1:end_index_1]\n",
    "\tX_half_batch_2 = X_train[start_index_2:end_index_2]\n",
    "\tX_train_multi = np.concatenate((X_half_batch_1,X_half_batch_2))\n",
    "\n",
    "\ty_half_batch_1 = y_train[start_index_1:end_index_1]\n",
    "\ty_half_batch_2 = y_train[start_index_2:end_index_2]\n",
    "\ty_train_multi = np.concatenate((y_half_batch_1,y_half_batch_2))\n",
    "\n",
    "\treturn\n",
    "\n",
    "\n",
    "def lbfgs_line_search_algorithm(sess,max_num_iter=max_num_iter):\n",
    "\ttolerance = 1E-5\n",
    "\n",
    "\tglobal gamma\n",
    "\tglobal g\n",
    "\n",
    "\tk = 0\n",
    "\t#-------- main loop ----------\n",
    "\twhile(True):\n",
    "\t\tprint('-'*60)\n",
    "\t\tprint('iteration: {}' .format(k))\n",
    "\n",
    "\t\tset_multi_batch(num_batch_in_data, k)\n",
    "\t\tsave_print_training_results(sess)\n",
    "\n",
    "\t\tg = eval_gradient_vec(sess)\t\n",
    "\t\tnorm_g = norm(g)\n",
    "\t\tprint('norm of g = {0:.4f}' .format(norm_g))\n",
    "\t\tif norm_g < tolerance:\n",
    "\t\t\tprint('-'*60)\n",
    "\t\t\tprint('gradient vanished')\n",
    "\t\t\tprint('convergence necessary but not sufficient condition') \n",
    "\t\t\tprint('--BREAK -- the trust region loop!')\n",
    "\t\t\tprint('-'*60)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif k >= max_num_iter:\n",
    "\t\t\tprint('reached to the max num iteration -- BREAK')\n",
    "\t\t\tbreak\t\n",
    "\t\t\n",
    "\t\tif k == 0:\n",
    "\t\t\t#p = backtracking_line_search(sess,g)\n",
    "\t\t\tp = -g\n",
    "\t\t\talpha = satisfy_wolfe_condition(sess, p)\n",
    "\t\t\tp = alpha*p\n",
    "\t\telse:\n",
    "\t\t\tp = lbfgs_line_search_subproblem_solver(sess, g)\n",
    "\n",
    "\t\tnew_loss = eval_aux_loss(sess,p) \n",
    "\t\t# we should call this function everytime before \n",
    "\t\t# evaluation of aux gradient\t\t\t\n",
    "\t\tnew_y = eval_y(sess)\n",
    "\t\tnew_s = p\n",
    "\t\tupdate_S_Y(new_s,new_y)\n",
    "\t\tgamma = (new_y.T @ new_y) / (new_s.T @ new_y)\n",
    "\t\tprint('gamma = {0:.4f}' .format(gamma))\n",
    "\t\tupdate_weights(sess,p)\n",
    "\t\tprint('weights are updated')\n",
    "\n",
    "\t\tglobal iter_num\n",
    "\t\titer_num = k\n",
    "\t\t\t\t\n",
    "\t\tk += 1\n",
    "\treturn\n",
    "\n",
    "\n",
    "def lbfgs_trust_region_algorithm(sess,max_num_iter=max_num_iter):\n",
    "\t#--------- LOOP PARAMS ------------\n",
    "\tdelta_hat = 3 # upper bound for trust region radius\n",
    "\t#max_num_iter = 1000 # max bunmber of trust region iterations\n",
    "\tdelta = np.zeros(max_num_iter+1)\n",
    "\tdelta[0] = delta_hat * 0.75\n",
    "\trho = np.zeros(max_num_iter) # true reduction / predicted reduction ratio\n",
    "\teta = 1/4 * 0.9 # eta \\in [0,1/4)\n",
    "\tnew_iteration = True\n",
    "\tnew_iteration_number = 0\n",
    "\ttolerance = 1E-5\n",
    "\n",
    "\tglobal gamma\n",
    "\tglobal g\n",
    "\n",
    "\tk = 0\n",
    "\t#-------- main loop ----------\n",
    "\twhile(True):\n",
    "\t\tprint('-'*60)\n",
    "\t\tprint('iteration: {}' .format(k))\n",
    "\t\t\n",
    "\t\tif new_iteration:\n",
    "\t\t\tset_multi_batch(num_batch_in_data, new_iteration_number)\n",
    "\t\t\tsave_print_training_results(sess)\n",
    "\n",
    "\t\tg = eval_gradient_vec(sess)\t\n",
    "\t\tnorm_g = norm(g)\n",
    "\t\tprint('norm of g = {0:.4f}' .format(norm_g))\n",
    "\t\tif norm_g < tolerance:\n",
    "\t\t\tprint('-'*60)\n",
    "\t\t\tprint('gradient vanished')\n",
    "\t\t\tprint('convergence necessary but not sufficient condition') \n",
    "\t\t\tprint('--BREAK -- the trust region loop!')\n",
    "\t\t\tprint('-'*60)\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif k >= max_num_iter:\n",
    "\t\t\tprint('reached to the max num iteration -- BREAK')\n",
    "\t\t\tbreak\t\n",
    "\t\t\n",
    "\t\tif new_iteration_number == 0:\n",
    "\t\t\tp = backtracking_line_search(sess,g)\n",
    "\t\t\t\n",
    "\t\t\t# we should call this function everytime before \n",
    "\t\t\t# evaluation of aux gradient\n",
    "\t\t\tnew_loss = eval_aux_loss(sess,p) \n",
    "\t\t\tnew_y = eval_y(sess)\n",
    "\t\t\tnew_s = p\n",
    "\t\t\tupdate_S_Y(new_s,new_y)\n",
    "\t\t\tgamma = (new_y.T @ new_y) / (new_s.T @ new_y)\n",
    "\t\t\tprint('initial gamma = {0:.4f}' .format(gamma))\n",
    "\t\t\tnew_iteration = True\n",
    "\t\t\tnew_iteration_number += 1\n",
    "\t\t\tupdate_weights(sess,p)\n",
    "\t\t\tprint('weights are updated')\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tp = lbfgs_trust_region_subproblem_solver(delta[k], g)\n",
    "\t\t\n",
    "\t\trho[k] = eval_reduction_ratio(sess, g, p)\n",
    "\t\tif rho[k] < 1/4:\n",
    "\t\t\tdelta[k+1] = 1/4 * delta[k]\n",
    "\t\t\tprint('shrinking trust region radius')\n",
    "\t\telse:\n",
    "\t\t\tif rho[k] > 3/4 and isclose( norm(p), delta[k] ):\n",
    "\t\t\t\tdelta[k+1] = min(2*delta[k], delta_hat)\n",
    "\t\t\t\tprint('expanding trust region radius')\n",
    "\t\t\telse:\n",
    "\t\t\t\tdelta[k+1] = delta[k]\n",
    "\n",
    "\t\tif rho[k] > eta:\n",
    "\t\t\tnew_y = eval_y(sess)\n",
    "\t\t\tnew_s = p\n",
    "\t\t\tupdate_S_Y(new_s,new_y)\n",
    "\t\t\tgamma = (new_y.T @ new_y) / (new_s.T @ new_y)\n",
    "\t\t\tprint('gamma = {0:.4f}' .format(gamma))\n",
    "\t\t\tif gamma < 0 or isclose(gamma,0):\n",
    "\t\t\t\tprint('WARNING! -- gamma is not stable')\n",
    "\t\t\tnew_iteration = True\n",
    "\t\t\tnew_iteration_number += 1\n",
    "\n",
    "\t\t\tupdate_weights(sess,p)\n",
    "\t\t\tprint('weights are updated')\n",
    "\t\telse:\n",
    "\t\t\tnew_iteration = False\n",
    "\t\t\tprint('-'*30)\n",
    "\t\t\tprint('No update in this iteration')\n",
    "\n",
    "\t\tglobal iter_num\n",
    "\t\titer_num = k\n",
    "\n",
    "\t\tk += 1\n",
    "\treturn\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(init)\n",
    "\n",
    "\tif method == 'trust-region':\n",
    "\t\tlbfgs_trust_region_algorithm(sess)\n",
    "\telif method == 'line-search':\n",
    "\t\tlbfgs_line_search_algorithm(sess)\n",
    "\telse:\n",
    "\t\tprint('Error! No proper method is defined')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "loop_time = end - start\n",
    "each_iteration_avg_time = loop_time / (iter_num+1)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "result_file_path = './results/results_experiment_FEB_23_' + str(method) + '_m_' \\\n",
    "\t\t\t\t\t\t\t+ str(m) + '_n_' + str(num_batch_in_data) + '.pkl'\n",
    "if use_whole_data:\n",
    "\tresult_file_path = './results/results_experiment_FEB_23_' + str(method) + '_m_' \\\n",
    "\t\t\t\t\t\t\t+ str(m) + '_n_2' + '.pkl'\n",
    "# Saving the objects:\n",
    "with open(result_file_path, 'wb') as f: \n",
    "\tpickle.dump([loss_train_results, loss_validation_results, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tloss_test_results], f)\n",
    "\tpickle.dump([accuracy_train_results, accuracy_validation_results, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taccuracy_test_results], f)\n",
    "\tpickle.dump([loop_time, each_iteration_avg_time], f)\n",
    "\n",
    "# import pickle\n",
    "# result_file_path = './results/results_experiment_' + str(method) + '_m_' \\\n",
    "# \t\t\t\t\t\t\t+ str(m) + '_n_' + str(num_batch_in_data) + '.pkl'\n",
    "# with open(result_file_path,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "# \tloss_train_results, loss_validation_results, loss_test_results = \\\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpickle.load(f)\n",
    "# \taccuracy_train_results,accuracy_validation_results, \\\n",
    "# \t\t\t\t\t\t\t\t\t\taccuracy_test_results = pickle.load(f)\n",
    "# \tloop_time, each_iteration_avg_time = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97225317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.10.9: C:\\Users\\friedele\\anaconda3+\\python.exe\n",
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n",
      "E0331 13:42:32.345481 17812 ultratb.py:152] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\app.py\", line 156, in parse_flags_with_usage\n",
      "    return FLAGS(args)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\flags\\_flagvalues.py\", line 650, in __call__\n",
      "    raise _exceptions.UnrecognizedFlagError(\n",
      "absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'f'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\friedele\\AppData\\Local\\Temp\\ipykernel_8320\\3421210488.py\", line 2, in <module>\n",
      "    test_util.main()\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\tensorflow_probability\\python\\internal\\test_util.py\", line 1331, in main\n",
      "    absltest.main(testLoader=_TestLoader())\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\testing\\absltest.py\", line 2057, in main\n",
      "    _run_in_app(run_tests, args, kwargs)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\testing\\absltest.py\", line 2162, in _run_in_app\n",
      "    app.run(main=main_function)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\app.py\", line 300, in run\n",
      "    args = _run_init(\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\app.py\", line 369, in _run_init\n",
      "    args = _register_and_parse_flags_with_usage(\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\app.py\", line 216, in _register_and_parse_flags_with_usage\n",
      "    args_to_main = flags_parser(original_argv)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\absl\\app.py\", line 166, in parse_flags_with_usage\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\friedele\\anaconda3+\\lib\\inspect.py\", line 1670, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mparse_flags_with_usage\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    155\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    649\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0m\u001b[0;32m    651\u001b[0m           name, value, suggestions=suggestions)\n",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8320\\3421210488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mtest_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\tensorflow_probability\\python\\internal\\test_util.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(jax_mode)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[0mtest_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInstallStackTraceHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m     \u001b[0mabsltest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestLoader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_TestLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\testing\\absltest.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2056\u001b[0m   \u001b[0mprint_python_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2057\u001b[1;33m   \u001b[0m_run_in_app\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_tests\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\testing\\absltest.py\u001b[0m in \u001b[0;36m_run_in_app\u001b[1;34m(function, args, kwargs)\u001b[0m\n\u001b[0;32m   2161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2162\u001b[1;33m     \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    299\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     args = _run_init(\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_run_init\u001b[1;34m(argv, flags_parser)\u001b[0m\n\u001b[0;32m    368\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_absl_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m   args = _register_and_parse_flags_with_usage(\n\u001b[0m\u001b[0;32m    370\u001b[0m       \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36m_register_and_parse_flags_with_usage\u001b[1;34m(argv, flags_parser)\u001b[0m\n\u001b[0;32m    215\u001b[0m   \u001b[0moriginal_argv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m   \u001b[0margs_to_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_argv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\absl\\app.py\u001b[0m in \u001b[0;36mparse_flags_with_usage\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pass --helpshort or --helpfull to see help on flags.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2068\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[0;32m   2069\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m-> 2070\u001b[1;33m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[0;32m   2071\u001b[0m                                                                      value))\n\u001b[0;32m   2072\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m             out_list = (\n\u001b[1;32m--> 629\u001b[1;33m                 self.structured_traceback(\n\u001b[0m\u001b[0;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0metb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3+\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  test_util.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993cfed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
