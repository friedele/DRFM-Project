{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Wh06oUdjUDhN9MTXmJQ7ovs--SZrDrRG","authorship_tag":"ABX9TyN6uxiZZdFFW3e65CAJIAf1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"8CIGs479QqRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, sys\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","# Load Modules\n","sys.path.append('/content/drive/MyDrive/Modules')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezMfGHDqrH74","executionInfo":{"status":"ok","timestamp":1682286991158,"user_tz":360,"elapsed":10800,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}},"outputId":"2c32934e-51b8-45e7-f899-d394b29667df"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Plot Routines"],"metadata":{"id":"MOsEo0ITvQ97"}},{"cell_type":"markdown","source":["# Class definitions\n","\n","The momentum method allows us to solve the gradient descent problem described above. Looking at the optimization trace above we might intuit that averaging gradients over the past would work well. After all, in the  x1  direction this will aggregate well-aligned gradients, thus increasing the distance we cover with every step. Conversely, in the x2  direction where gradients oscillate, an aggregate gradient will reduce step size due to oscillations that cancel each other out. Using  vt  instead of the gradient  gt  yields the following update equations:\n","\n","vtxt←βvt−1+gt,t−1,←xt−1−ηtvt. \n","\n","Note that for  β=0  we recover regular gradient descent. Before delving deeper into the mathematical properties let's have a quick look at how the algorithm behaves in practice."],"metadata":{"id":"e_yJkvyaCS8G"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.elu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = F.elu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.elu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=4):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def ResNet9():\n","    return ResNet(BasicBlock, [1,1,1,1])\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])"],"metadata":{"id":"q75ImJEQCbH_","executionInfo":{"status":"ok","timestamp":1682287021178,"user_tz":360,"elapsed":4062,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import random\n","def plot_transformed_images(image_paths, transform, n=3, seed=42):\n","    \"\"\"Plots a series of random images from image_paths.\n","\n","    Will open n image paths from image_paths, transform them\n","    with transform and plot them side by side.\n","\n","    Args:\n","        image_paths (list): List of target image paths. \n","        transform (PyTorch Transforms): Transforms to apply to images.\n","        n (int, optional): Number of images to plot. Defaults to 3.\n","        seed (int, optional): Random seed for the random generator. Defaults to 42.\n","    \"\"\"\n","    random.seed(seed)\n","    random_image_paths = random.sample(image_paths, k=n)\n","    for image_path in random_image_paths:\n","        with Image.open(image_path) as f:\n","            fig, ax = plt.subplots(1, 2)\n","            ax[0].imshow(f) \n","            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n","            ax[0].axis(\"off\")\n","\n","            # Transform and plot image\n","            # Note: permute() will change shape of image to suit matplotlib \n","            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n","            transformed_image = transform(f).permute(1, 2, 0) \n","            ax[1].imshow(transformed_image) \n","            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n","            ax[1].axis(\"off\")\n","\n","            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n"],"metadata":{"id":"3xUGsPtJfs87","executionInfo":{"status":"ok","timestamp":1682287021179,"user_tz":360,"elapsed":6,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCYmcw_xCRK8","executionInfo":{"status":"ok","timestamp":1682287021179,"user_tz":360,"elapsed":5,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# **Load Datasets**"],"metadata":{"id":"G8hXuICSjNEM"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from pathlib import Path\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","data_transform=transforms.Compose(\n","   [ transforms.Resize(size=(32,32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n","\n","# Setup path to data folder\n","# Dataset Parameters\n","data_path = Path('/content/drive/MyDrive/Images/') # The classes or folders under here will be used\n","image_path = data_path / \"allTgts(64x64)\"\n","# If the image folder doesn't exist, download it and prepare it... \n","if image_path.is_dir():\n","    print(f\"{image_path} directory exists.\")\n","else:\n","    print(f\"Did not find {image_path} directory\")\n","    \n","# Setup train and testing paths\n","train_dir = image_path / \"training\"\n","test_dir = image_path / \"test\"\n","\n","# Use ImageFolder to create dataset(s)\n","trainset = datasets.ImageFolder(root=train_dir, # target folder of images\n","                                  transform=data_transform, # transforms to perform on data (images)\n","                                  target_transform=None) # transforms to perform on labels (if necessary)\n","\n","testset = datasets.ImageFolder(root=test_dir, \n","                                 transform=data_transform) \n","\n","print(f\"Train data:\\n{trainset}\\nTest data:\\n{testset}\")\n","\n","# Get class names as a list\n","classes = trainset.classes\n"," \n","# Turn train and test Datasets into DataLoaders\n","trainloader = DataLoader(dataset=trainset, \n","                              batch_size=default_batch, # how many samples per batch?\n","                              num_workers=2, # how many subprocesses to use for data loading? (higher = more)\n","                              shuffle=True) # shuffle the data?\n","\n","testloader = DataLoader(dataset=testset, \n","                             batch_size=default_batch, \n","                             num_workers=2, \n","                             shuffle=False) # don't usually need to shuffle testing data\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_Y5nUl1jQPv","executionInfo":{"status":"ok","timestamp":1680902415750,"user_tz":360,"elapsed":719,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}},"outputId":"a787da92-7079-4dbe-bcea-bb4879da395d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Images/allTgts(64x64) directory exists.\n","Train data:\n","Dataset ImageFolder\n","    Number of datapoints: 3199\n","    Root location: /content/drive/MyDrive/Images/allTgts(64x64)/training\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","           )\n","Test data:\n","Dataset ImageFolder\n","    Number of datapoints: 804\n","    Root location: /content/drive/MyDrive/Images/allTgts(64x64)/test\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","           )\n"]}]},{"cell_type":"markdown","source":["# **Original L-BFGS code that works** "],"metadata":{"id":"LoXFG3wQWXOn"}},{"cell_type":"code","source":["# Original Code\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import math\n","import time\n","from lbfgsnew import LBFGSNew \n","\n","# Does not error but does not finish.  I suppose this probably takes awhile to train.\n","# https://github.com/nlesc-dirac/pytorch\n","# (try to) use a GPU for computation?\n","use_cuda=True\n","if use_cuda and torch.cuda.is_available():\n","  mydevice=torch.device('cuda')\n","  print ('Using CUDA')\n","else:\n","  mydevice=torch.device('cpu')\n","  print ('Using CPUs')\n","\n","# try replacing relu with elu\n","torch.manual_seed(69)\n","default_batch=128 # no. of batches per epoch 50000/default_batch\n","batches_for_report=10#\n","epochs = 20\n","\n","transform=transforms.Compose(\n","   [transforms.ToTensor(),\n","     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n","\n","# Using CIFAR-10 images at (3,32,32)\n","trainset=torchvision.datasets.CIFAR10(root='./torchdata', train=True,\n","    download=True, transform=transform)\n","\n","trainloader=torch.utils.data.DataLoader(trainset, batch_size=default_batch,\n","    shuffle=True, num_workers=2)\n","\n","testset=torchvision.datasets.CIFAR10(root='./torchdata', train=False,\n","    download=True, transform=transform)\n","\n","testloader=torch.utils.data.DataLoader(testset, batch_size=default_batch,\n","    shuffle=False, num_workers=0)\n","\n","\n","classes=('plane', 'car', 'bird', 'cat', \n","  'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","'''ResNet in PyTorch.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n"," \n","From: https://github.com/kuangliu/pytorch-cifar\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.elu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = F.elu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.elu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.elu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def ResNet9():\n","    return ResNet(BasicBlock, [1,1,1,1])\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","# enable this to use wide ResNet\n","wide_resnet=False\n","if not wide_resnet:\n","  net=ResNet9().to(mydevice)\n","  print(\"Using resnet9\")\n","else:\n","  # use wide residual net https://arxiv.org/abs/1605.07146\n","  net=torchvision.models.resnet.wide_resnet50_2().to(mydevice)\n","  print(\"Using resnet50\")\n","\n","\n","#####################################################\n","def verification_error_check(net):\n","   correct=0\n","   total=0\n","   for data in testloader:\n","     images,labels=data\n","     outputs=net(Variable(images).to(mydevice))\n","     _,predicted=torch.max(outputs.data,1)\n","     correct += (predicted==labels.to(mydevice)).sum()\n","     total += labels.size(0)\n","\n","   return 100*correct//total\n","#####################################################\n","\n","lambda1=0.000001\n","lambda2=0.001\n","\n","# loss function and optimizer\n","import torch.optim as optim\n","#from lbfgsnew import LBFGSNew # custom optimizer\n","criterion=nn.CrossEntropyLoss()\n","#optimizer=optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","#optimizer=optim.Adam(net.parameters(), lr=0.001)\n","optimizer=LBFGSNew(net.parameters(), history_size=7, max_iter=2, line_search_fn=True,batch_mode=True)\n","\n","load_model=False\n","# update from a saved model \n","if load_model:\n","  checkpoint=torch.load('./res18.model',map_location=mydevice)\n","  net.load_state_dict(checkpoint['model_state_dict'])\n","  net.train() # initialize for training (BN,dropout)\n","\n","start_time=time.time()\n","use_lbfgs=True\n","# train network\n","for epoch in range(epochs):  # Setting to 20 Epochs\n","  running_loss=0.0\n","  for i,data in enumerate(trainloader,0):\n","    # get the inputs\n","    inputs,labels=data\n","    # wrap them in variable\n","    inputs,labels=Variable(inputs).to(mydevice),Variable(labels).to(mydevice)\n","\n","    if not use_lbfgs:\n","     # zero gradients\n","     optimizer.zero_grad()\n","     print(\"Not using LBFGS!\")\n","     # forward+backward optimize\n","     outputs=net(inputs)\n","     loss=criterion(outputs,labels)\n","     loss.backward()\n","     optimizer.step()\n","    else:\n","      print(\"Using LBFGS!\")\n","      if not wide_resnet:\n","        layer1=torch.cat([x.view(-1) for x in net.layer1.parameters()])\n","        layer2=torch.cat([x.view(-1) for x in net.layer2.parameters()])\n","        layer3=torch.cat([x.view(-1) for x in net.layer3.parameters()])\n","        layer4=torch.cat([x.view(-1) for x in net.layer4.parameters()])\n","\n","      def closure():\n","        if torch.is_grad_enabled():\n","         optimizer.zero_grad()\n","         #print(\"Using zero gradient\")\n","        outputs=net(inputs)\n","        if not wide_resnet:\n","          l1_penalty=lambda1*(torch.norm(layer1,1)+torch.norm(layer2,1)+torch.norm(layer3,1)+torch.norm(layer4,1))\n","          l2_penalty=lambda2*(torch.norm(layer1,2)+torch.norm(layer2,2)+torch.norm(layer3,2)+torch.norm(layer4,2))\n","          loss=criterion(outputs,labels)+l1_penalty+l2_penalty\n","          #print(\"Apply penalty in Loss function\")\n","        else:\n","          l1_penalty=0\n","          l2_penalty=0\n","          loss=criterion(outputs,labels)\n","        if loss.requires_grad:\n","          loss.backward()\n","          #print('loss %f l1 %f l2 %f'%(loss,l1_penalty,l2_penalty))\n","        return loss\n","      optimizer.step(closure)\n","    # only for diagnostics\n","    outputs=net(inputs)\n","    loss=criterion(outputs,labels)\n","    running_loss +=loss.data.item()\n","\n","    if math.isnan(loss.data.item()):\n","       print('loss became nan at %d'%i)\n","       break\n","\n","    # print statistics\n","    if i%(batches_for_report) == (batches_for_report-1): # after every 'batches_for_report'\n","      print('%f: [%d, %5d] loss: %.5f accuracy: %.3f'%\n","         (time.time()-start_time,epoch+1,i+1,running_loss/batches_for_report,\n","         verification_error_check(net)))\n","      running_loss=0.0\n","\n","print('Finished Training')\n","\n","\n","# save model (and other extra items)\n","torch.save({\n","            'model_state_dict':net.state_dict(),\n","            'epoch':epoch,\n","            'optimizer_state_dict':optimizer.state_dict(),\n","            'running_loss':running_loss,\n","           },'./res.model')\n","\n","\n","# whole dataset\n","correct=0\n","total=0\n","for data in trainloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   \n","print('Accuracy of the network on the %d train images: %d %%'%\n","    (total,100*correct//total))\n","\n","correct=0\n","total=0\n","for data in testloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   \n","print('Accuracy of the network on the %d test images: %d %%'%\n","    (total,100*correct//total))\n","\n","\n","class_correct=list(0. for i in range(10))\n","class_total=list(0. for i in range(10))\n","for data in testloader:\n","  images,labels=data\n","  outputs=net(Variable(images).to(mydevice)).cpu()\n","  _,predicted=torch.max(outputs.data,1)\n","  c=(predicted==labels).squeeze()\n","  for i in range(4):\n","    label=labels[i]\n","    class_correct[label] += c[i]\n","    class_total[label] += 1\n","\n","for i in range(10):\n","  print('Accuracy of %5s : %2d %%' %\n","    (classes[i],100*float(class_correct[i])/float(class_total[i])))\n"],"metadata":{"id":"wuuhEuVntq0q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d9f0e3b-76c3-41ca-fc1c-5bbd0ea080ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./torchdata/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 103233483.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./torchdata/cifar-10-python.tar.gz to ./torchdata\n","Files already downloaded and verified\n","Using resnet9\n","11.332979: [1,    10] loss: 2.24368 accuracy: 19.000\n","17.497870: [1,    20] loss: 1.84222 accuracy: 28.000\n","23.134989: [1,    30] loss: 1.62061 accuracy: 34.000\n","28.792189: [1,    40] loss: 1.60208 accuracy: 31.000\n","34.864778: [1,    50] loss: 1.63901 accuracy: 37.000\n","40.413222: [1,    60] loss: 1.52440 accuracy: 31.000\n","47.001148: [1,    70] loss: 1.56478 accuracy: 29.000\n","52.600640: [1,    80] loss: 1.49929 accuracy: 40.000\n","58.895788: [1,    90] loss: 1.40106 accuracy: 41.000\n","64.839242: [1,   100] loss: 1.38861 accuracy: 37.000\n","70.679908: [1,   110] loss: 1.42125 accuracy: 36.000\n","76.555209: [1,   120] loss: 1.38899 accuracy: 43.000\n","82.183238: [1,   130] loss: 1.39449 accuracy: 33.000\n","88.617739: [1,   140] loss: 1.37601 accuracy: 43.000\n","94.166793: [1,   150] loss: 1.28860 accuracy: 39.000\n","100.718268: [1,   160] loss: 1.37014 accuracy: 42.000\n","106.470933: [1,   170] loss: 1.24627 accuracy: 34.000\n","112.656656: [1,   180] loss: 1.37624 accuracy: 44.000\n","118.340889: [1,   190] loss: 1.27803 accuracy: 28.000\n","124.011628: [1,   200] loss: 1.48688 accuracy: 43.000\n","130.449811: [1,   210] loss: 1.27849 accuracy: 42.000\n","136.195310: [1,   220] loss: 1.25429 accuracy: 46.000\n","142.792473: [1,   230] loss: 1.22309 accuracy: 46.000\n","148.481952: [1,   240] loss: 1.23061 accuracy: 49.000\n","154.920050: [1,   250] loss: 1.18188 accuracy: 46.000\n","160.597557: [1,   260] loss: 1.18278 accuracy: 46.000\n","166.609278: [1,   270] loss: 1.09577 accuracy: 43.000\n","172.388356: [1,   280] loss: 1.15055 accuracy: 46.000\n","178.113046: [1,   290] loss: 1.10985 accuracy: 51.000\n","184.409343: [1,   300] loss: 1.04068 accuracy: 46.000\n","190.121057: [1,   310] loss: 1.08223 accuracy: 48.000\n","196.932065: [1,   320] loss: 1.08543 accuracy: 51.000\n","202.576072: [1,   330] loss: 1.03238 accuracy: 49.000\n","208.871355: [1,   340] loss: 1.04125 accuracy: 49.000\n","214.532643: [1,   350] loss: 1.08931 accuracy: 53.000\n","220.261811: [1,   360] loss: 1.13732 accuracy: 49.000\n","226.462428: [1,   370] loss: 1.06500 accuracy: 48.000\n","232.217650: [1,   380] loss: 1.02385 accuracy: 50.000\n","239.031745: [1,   390] loss: 0.96229 accuracy: 48.000\n","245.017325: [2,    10] loss: 0.98385 accuracy: 50.000\n","251.672016: [2,    20] loss: 0.95046 accuracy: 50.000\n","257.446213: [2,    30] loss: 0.93999 accuracy: 52.000\n","264.001914: [2,    40] loss: 0.87557 accuracy: 53.000\n","269.945056: [2,    50] loss: 0.82997 accuracy: 52.000\n","276.141898: [2,    60] loss: 0.87284 accuracy: 47.000\n","282.061327: [2,    70] loss: 0.89578 accuracy: 54.000\n","287.895901: [2,    80] loss: 0.82874 accuracy: 53.000\n","294.389460: [2,    90] loss: 0.92376 accuracy: 57.000\n","300.163610: [2,   100] loss: 0.82830 accuracy: 55.000\n","306.926336: [2,   110] loss: 0.80150 accuracy: 54.000\n","312.672658: [2,   120] loss: 0.76788 accuracy: 55.000\n","319.283621: [2,   130] loss: 0.81416 accuracy: 47.000\n","325.004767: [2,   140] loss: 0.83960 accuracy: 56.000\n","331.197476: [2,   150] loss: 0.80413 accuracy: 58.000\n","337.122825: [2,   160] loss: 0.77958 accuracy: 59.000\n","343.033913: [2,   170] loss: 0.74106 accuracy: 59.000\n","349.595420: [2,   180] loss: 0.75462 accuracy: 53.000\n","355.481504: [2,   190] loss: 0.77029 accuracy: 57.000\n","362.161244: [2,   200] loss: 0.72887 accuracy: 59.000\n","367.897868: [2,   210] loss: 0.76726 accuracy: 55.000\n","374.533331: [2,   220] loss: 0.72804 accuracy: 55.000\n","380.282320: [2,   230] loss: 0.73920 accuracy: 60.000\n","386.518702: [2,   240] loss: 0.86790 accuracy: 60.000\n","392.286383: [2,   250] loss: 0.74935 accuracy: 62.000\n","398.090437: [2,   260] loss: 0.69728 accuracy: 60.000\n","404.392146: [2,   270] loss: 0.71202 accuracy: 60.000\n","410.140129: [2,   280] loss: 0.70536 accuracy: 54.000\n","416.955383: [2,   290] loss: 0.68941 accuracy: 62.000\n","422.664755: [2,   300] loss: 0.68740 accuracy: 62.000\n","429.184714: [2,   310] loss: 0.62757 accuracy: 62.000\n","434.915029: [2,   320] loss: 0.65881 accuracy: 58.000\n","441.082128: [2,   330] loss: 0.69389 accuracy: 60.000\n","446.990868: [2,   340] loss: 0.69196 accuracy: 61.000\n","452.778117: [2,   350] loss: 0.62129 accuracy: 58.000\n","459.177520: [2,   360] loss: 0.64159 accuracy: 63.000\n","465.059184: [2,   370] loss: 0.65068 accuracy: 60.000\n","471.858639: [2,   380] loss: 0.69962 accuracy: 65.000\n","477.616975: [2,   390] loss: 0.67461 accuracy: 62.000\n","484.602742: [3,    10] loss: 0.58362 accuracy: 59.000\n","490.333001: [3,    20] loss: 0.57891 accuracy: 62.000\n","496.769766: [3,    30] loss: 0.57297 accuracy: 62.000\n","502.553900: [3,    40] loss: 0.60422 accuracy: 62.000\n","508.359807: [3,    50] loss: 0.60259 accuracy: 61.000\n","514.484394: [3,    60] loss: 0.57912 accuracy: 64.000\n","520.167869: [3,    70] loss: 0.61969 accuracy: 59.000\n","526.784490: [3,    80] loss: 0.60181 accuracy: 62.000\n","532.504868: [3,    90] loss: 0.58403 accuracy: 60.000\n","539.128617: [3,   100] loss: 0.59038 accuracy: 64.000\n","544.862985: [3,   110] loss: 0.57659 accuracy: 65.000\n","551.022473: [3,   120] loss: 0.49620 accuracy: 65.000\n","556.810063: [3,   130] loss: 0.55906 accuracy: 64.000\n","562.612209: [3,   140] loss: 0.55662 accuracy: 64.000\n","569.115656: [3,   150] loss: 0.49555 accuracy: 64.000\n","574.958166: [3,   160] loss: 0.54186 accuracy: 63.000\n","581.531173: [3,   170] loss: 0.55006 accuracy: 65.000\n","587.197758: [3,   180] loss: 0.52380 accuracy: 65.000\n","593.816321: [3,   190] loss: 0.51768 accuracy: 66.000\n","599.730094: [3,   200] loss: 0.51811 accuracy: 60.000\n","606.108388: [3,   210] loss: 0.54966 accuracy: 65.000\n","611.834943: [3,   220] loss: 0.50507 accuracy: 67.000\n","617.590530: [3,   230] loss: 0.52263 accuracy: 64.000\n","623.810595: [3,   240] loss: 0.51174 accuracy: 67.000\n","629.515343: [3,   250] loss: 0.55891 accuracy: 65.000\n","636.246909: [3,   260] loss: 0.53433 accuracy: 67.000\n","641.940914: [3,   270] loss: 0.46296 accuracy: 66.000\n","648.459268: [3,   280] loss: 0.49157 accuracy: 66.000\n","654.240968: [3,   290] loss: 0.43980 accuracy: 66.000\n","660.373682: [3,   300] loss: 0.51482 accuracy: 65.000\n","666.123894: [3,   310] loss: 0.50189 accuracy: 68.000\n","671.868878: [3,   320] loss: 0.49311 accuracy: 67.000\n","678.234379: [3,   330] loss: 0.51565 accuracy: 68.000\n","683.965426: [3,   340] loss: 0.49515 accuracy: 67.000\n","690.632952: [3,   350] loss: 0.48420 accuracy: 66.000\n","696.380830: [3,   360] loss: 0.46821 accuracy: 62.000\n","702.870080: [3,   370] loss: 0.53654 accuracy: 67.000\n","708.601144: [3,   380] loss: 0.47704 accuracy: 66.000\n","714.742085: [3,   390] loss: 0.47157 accuracy: 67.000\n","720.989856: [4,    10] loss: 0.48806 accuracy: 69.000\n","726.740785: [4,    20] loss: 0.41063 accuracy: 68.000\n","732.965878: [4,    30] loss: 0.41175 accuracy: 68.000\n","738.722245: [4,    40] loss: 0.38003 accuracy: 67.000\n","745.350594: [4,    50] loss: 0.43718 accuracy: 68.000\n","751.089433: [4,    60] loss: 0.43042 accuracy: 68.000\n","757.582313: [4,    70] loss: 0.41858 accuracy: 69.000\n","763.266940: [4,    80] loss: 0.40308 accuracy: 67.000\n","769.275952: [4,    90] loss: 0.40962 accuracy: 69.000\n","775.202364: [4,   100] loss: 0.39563 accuracy: 69.000\n","780.932283: [4,   110] loss: 0.37620 accuracy: 66.000\n","787.435750: [4,   120] loss: 0.41388 accuracy: 69.000\n","793.156197: [4,   130] loss: 0.38792 accuracy: 70.000\n","799.963894: [4,   140] loss: 0.41134 accuracy: 69.000\n","805.629421: [4,   150] loss: 0.37057 accuracy: 68.000\n","812.048117: [4,   160] loss: 0.36929 accuracy: 66.000\n","817.712156: [4,   170] loss: 0.38643 accuracy: 70.000\n","823.650997: [4,   180] loss: 0.38422 accuracy: 68.000\n","829.643753: [4,   190] loss: 0.40869 accuracy: 69.000\n","835.434151: [4,   200] loss: 0.43144 accuracy: 68.000\n","841.991927: [4,   210] loss: 0.40557 accuracy: 69.000\n","847.664759: [4,   220] loss: 0.41800 accuracy: 71.000\n","854.241430: [4,   230] loss: 0.40210 accuracy: 69.000\n","860.039168: [4,   240] loss: 0.38589 accuracy: 70.000\n","866.280092: [4,   250] loss: 0.36918 accuracy: 70.000\n","872.044432: [4,   260] loss: 0.42450 accuracy: 71.000\n","877.864670: [4,   270] loss: 0.38531 accuracy: 70.000\n","884.002313: [4,   280] loss: 0.35743 accuracy: 70.000\n","889.700990: [4,   290] loss: 0.33417 accuracy: 69.000\n","896.280772: [4,   300] loss: 0.37112 accuracy: 70.000\n","902.024755: [4,   310] loss: 0.35468 accuracy: 71.000\n","908.652704: [4,   320] loss: 0.35474 accuracy: 71.000\n","914.368699: [4,   330] loss: 0.36030 accuracy: 71.000\n","920.590189: [4,   340] loss: 0.34699 accuracy: 71.000\n","926.273760: [4,   350] loss: 0.36629 accuracy: 67.000\n","932.067160: [4,   360] loss: 0.38403 accuracy: 71.000\n","938.322207: [4,   370] loss: 0.35144 accuracy: 71.000\n","943.994781: [4,   380] loss: 0.34719 accuracy: 71.000\n","950.608395: [4,   390] loss: 0.33273 accuracy: 73.000\n","956.645348: [5,    10] loss: 0.30582 accuracy: 72.000\n","963.297426: [5,    20] loss: 0.30848 accuracy: 70.000\n","969.014017: [5,    30] loss: 0.32248 accuracy: 70.000\n","975.103283: [5,    40] loss: 0.30885 accuracy: 70.000\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qTsyShBqsB58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#L-BFGS DRFM v1 \n"," \n","\n","*   Accuracy of the network on the 3199 train images: 99 %\n","*   Accuracy of the network on the 804 test images: 39 %\n","*   Accuracy of n01-randomTgts : 50 %\n","*   Accuracy of n02-rangeTgts : 62 %\n","*   Accuracy of n03-dopTgts : 75 %\n","*   Accuracy of n04-combinedTgts : 75 %\n"],"metadata":{"id":"pL-7L6bQWgL8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2MAX8gCzrE_3","executionInfo":{"status":"error","timestamp":1681337272092,"user_tz":360,"elapsed":21250,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}},"outputId":"33984d60-6e91-4265-f52f-3254400325a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA\n","/content/drive/MyDrive/Images/reducedTgts(64x64) directory exists.\n","Train data:\n","Dataset ImageFolder\n","    Number of datapoints: 80\n","    Root location: /content/drive/MyDrive/Images/reducedTgts(64x64)/training\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","           )\n","Test data:\n","Dataset ImageFolder\n","    Number of datapoints: 44\n","    Root location: /content/drive/MyDrive/Images/reducedTgts(64x64)/test\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","           )\n","<bound method Module.parameters of ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=4, bias=True)\n",")>\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-87b03e27a229>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;31m#print('loss %f l1 %f l2 %f'%(loss,l1_penalty,l2_penalty))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;31m# only for diagnostics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'g_Ok'"]}],"source":["import math\n","import time\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from pathlib import Path\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# https://github.com/nlesc-dirac/pytorch\n","# (try to) use a GPU for computation?\n","use_cuda=True\n","if use_cuda and torch.cuda.is_available():\n","  mydevice=torch.device('cuda')\n","  print ('Using CUDA')\n","else:\n","  mydevice=torch.device('cpu')\n","  print ('Using CPUs')\n","\n","# try replacing relu with elu\n","torch.manual_seed(69)\n","default_batch=128 # no. of batches per epoch 50000/default_batch\n","batches_for_report=10 #\n","epochs = 100\n","\n","data_transform=transforms.Compose(\n","   [ transforms.Resize(size=(32,32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n","\n","# Setup path to data folder\n","# Dataset Parameters\n","data_path = Path('/content/drive/MyDrive/Images/') # The classes or folders under here will be used\n","image_path = data_path / \"reducedTgts(64x64)\"\n","# If the image folder doesn't exist, download it and prepare it... \n","if image_path.is_dir():\n","    print(f\"{image_path} directory exists.\")\n","else:\n","    print(f\"Did not find {image_path} directory\")\n","    \n","# Setup train and testing paths\n","train_dir = image_path / \"training\"\n","test_dir = image_path / \"test\"\n","\n","# Use ImageFolder to create dataset(s)\n","trainset = datasets.ImageFolder(root=train_dir, # target folder of images\n","                                  transform=data_transform, # transforms to perform on data (images)\n","                                  target_transform=None) # transforms to perform on labels (if necessary)\n","\n","testset = datasets.ImageFolder(root=test_dir, \n","                                 transform=data_transform) \n","\n","print(f\"Train data:\\n{trainset}\\nTest data:\\n{testset}\")\n","\n","# Get class names as a list\n","classes = trainset.classes\n","num_classes= len(classes)\n"," \n","# Turn train and test Datasets into DataLoaders\n","trainloader = DataLoader(dataset=trainset, \n","                              batch_size=default_batch, # how many samples per batch?\n","                              num_workers=2, # how many subprocesses to use for data loading? (higher = more)\n","                              shuffle=True) # shuffle the data?\n","\n","testloader = DataLoader(dataset=testset, \n","                             batch_size=default_batch, \n","                             num_workers=2, \n","                             shuffle=False) # don't usually need to shuffle testing data\n","\n","\n","'''ResNet in PyTorch.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n"," \n","From: https://github.com/kuangliu/pytorch-cifar'''\n","# enable this to use wide ResNet\n","wide_resnet=False\n","\n","if not wide_resnet:\n","  net=ResNet9().to(mydevice) # Fewest layers using Resnet9\n","else:\n","  # use wide residual net https://arxiv.org/abs/1605.07146\n","  net=torchvision.models.resnet.resnet152().to(mydevice)\n","\n","#####################################################\n","def verification_error_check(net):\n","   correct=0\n","   total=0\n","   for data in testloader:\n","     images,labels=data\n","     outputs=net(Variable(images).to(mydevice))\n","     _,predicted=torch.max(outputs.data,1)\n","     correct += (predicted==labels.to(mydevice)).sum()\n","     total += labels.size(0)\n","\n","   return 100*correct//total\n","#####################################################\n","\n","lambda1=0.000001\n","lambda2=0.001\n","\n","# loss function and optimizer\n","import torch.optim as optim\n","from LBFGS import LBFGS # custom optimizer\n","criterion=nn.CrossEntropyLoss()\n","#optimizer=optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","#optimizer=optim.Adam(net.parameters(), lr=0.001)\n","#optimizer = LBFGS(net.parameters(), history_size=7, max_iter=2, line_search_fn=True,batch_mode=True)\n","optimizer = LBFGS(net.parameters(), history_size=7)\n","print(net.parameters)\n","\n","### load_model=False\n","# # update from a saved model \n","# if load_model:\n","#   checkpoint=torch.load('./res18.model',map_location=mydevice)\n","#   net.load_state_dict(checkpoint['model_state_dict'])\n","#   net.train() # initialize for training (BN,dropout)\n","\n","start_time=time.time()\n","use_lbfgs=True\n","# train network\n","for epoch in range(epochs ):\n","  running_loss=0.0\n","  for i,data in enumerate(trainloader,0):\n","    # get the inputs\n","    inputs,labels=data\n","    # wrap them in variable\n","    inputs,labels=Variable(inputs).to(mydevice),Variable(labels).to(mydevice)\n","\n","    if not use_lbfgs:\n","     # zero gradients\n","     optimizer.zero_grad()\n","     # forward+backward optimize\n","     outputs=net(inputs)\n","     loss=criterion(outputs,labels)\n","     loss.backward()\n","     optimizer.step()\n","    else:\n","      if not wide_resnet:\n","        layer1=torch.cat([x.view(-1) for x in net.layer1.parameters()])\n","        layer2=torch.cat([x.view(-1) for x in net.layer2.parameters()])\n","        layer3=torch.cat([x.view(-1) for x in net.layer3.parameters()])\n","        layer4=torch.cat([x.view(-1) for x in net.layer4.parameters()])\n","\n","      def closure():\n","        if torch.is_grad_enabled():\n","         optimizer.zero_grad()   # Doing this\n","        outputs=net(inputs)\n","        if not wide_resnet:  # Doing this\n","          l1_penalty=lambda1*(torch.norm(layer1,1)+torch.norm(layer2,1)+torch.norm(layer3,1)+torch.norm(layer4,1))\n","          l2_penalty=lambda2*(torch.norm(layer1,2)+torch.norm(layer2,2)+torch.norm(layer3,2)+torch.norm(layer4,2))\n","          loss=criterion(outputs,labels)+l1_penalty+l2_penalty\n","        else:\n","          l1_penalty=0\n","          l2_penalty=0\n","          loss=criterion(outputs,labels)\n","        if loss.requires_grad:  # Doing this\n","          loss.backward()\n","          #print('loss %f l1 %f l2 %f'%(loss,l1_penalty,l2_penalty))\n","        return loss\n","      optimizer.step(closure)\n","    # only for diagnostics\n","    outputs=net(inputs)\n","    loss=criterion(outputs,labels)\n","    running_loss +=loss.data.item()\n","\n","    if math.isnan(loss.data.item()):\n","       print('loss became nan at %d'%i)\n","       break\n","\n","    # print statistics\n","    if i%(batches_for_report) == (batches_for_report-1): # after every 'batches_for_report'\n","      print('%f: [%d, %5d] loss: %.5f accuracy: %.3f'%\n","         (time.time()-start_time,epoch+1,i+1,running_loss/batches_for_report,\n","         verification_error_check(net)))\n","      running_loss=0.0\n","\n","print('Finished Training')\n","\n","\n","# save model (and other extra items)\n","torch.save({\n","            'model_state_dict':net.state_dict(),\n","            'epoch':epoch,\n","            'optimizer_state_dict':optimizer.state_dict(),\n","            'running_loss':running_loss,\n","           },'./res.model')\n","\n","\n","# whole dataset\n","correct=0\n","total=0\n","for data in trainloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   \n","print('Accuracy of the network on the %d train images: %d %%'%\n","    (total,100*correct//total))\n","\n","correct=0\n","total=0\n","for data in testloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   \n","print('Accuracy of the network on the %d test images: %d %%'%\n","    (total,100*correct//total))\n","\n","\n","class_correct=list(0. for i in range(num_classes))\n","class_total=list(0. for i in range(num_classes))\n","for data in testloader:\n","  images,labels=data\n","  outputs=net(Variable(images).to(mydevice)).cpu()\n","  _,predicted=torch.max(outputs.data,1)\n","  c=(predicted==labels).squeeze()\n","  for i in range(4):\n","    label=labels[i]\n","    class_correct[label] += c[i]\n","    class_total[label] += 1\n","\n","for i in range(num_classes):\n","  print('Accuracy of %5s : %2d %%' %\n","    (classes[i],100*float(class_correct[i])/float(class_total[i])))"]},{"cell_type":"markdown","source":["# L-BFGS DRFM v2"],"metadata":{"id":"tybAdfhluztC"}},{"cell_type":"code","source":["import math\n","import time\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from pathlib import Path\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# https://github.com/nlesc-dirac/pytorch\n","\n","use_cuda=True\n","if use_cuda and torch.cuda.is_available():\n","  mydevice=torch.device('cuda')\n","  print ('Using CUDA')\n","else:\n","  mydevice=torch.device('cpu')\n","  print ('Using CPUs')\n","\n","# try replacing relu with elu\n","torch.manual_seed(69)\n","default_batch=128 # no. of batches per epoch 50000/default_batch\n","batches_for_report=10 #\n","epochs = 10\n","\n","# enable this to use wide ResNet\n","wide_resnet=False\n","\n","if not wide_resnet:\n","  net=ResNet9().to(mydevice) # Fewest layers using Resnet9\n","else:\n","  # use wide residual net https://arxiv.org/abs/1605.07146\n","  net=torchvision.models.resnet.wide_resnet50_2().to(mydevice)\n","\n","#####################################################\n","def verification_error_check(net):\n","   correct=0\n","   total=0\n","   for data in testloader:\n","     images,labels=data\n","     outputs=net(Variable(images).to(mydevice))\n","     _,predicted=torch.max(outputs.data,1)\n","     correct += (predicted==labels.to(mydevice)).sum()\n","     total += labels.size(0)\n","\n","   return 100*correct//total\n","#####################################################\n","\n","lambda1=0.000001\n","lambda2=0.001\n","\n","# loss function and optimizer\n","import torch.optim as optim\n","from lbfgsnew import LBFGSNew # custom optimizer\n","criterion=nn.CrossEntropyLoss()\n","#optimizer=optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","#optimizer=optim.Adam(net.parameters(), lr=0.001)\n","optimizer = LBFGSNew(net.parameters(), history_size=7, max_iter=2, line_search_fn=True,batch_mode=True)\n","print(net.parameters)\n","\n","### load_model=False\n","# # update from a saved model \n","# if load_model:\n","#   checkpoint=torch.load('./res18.model',map_location=mydevice)\n","#   net.load_state_dict(checkpoint['model_state_dict'])\n","#   net.train() # initialize for training (BN,dropout)\n","\n","start_time=time.time()\n","use_lbfgs=True\n","# train network\n","for epoch in range(epochs ):\n","  running_loss=0.0\n","  for i,data in enumerate(trainloader,0):\n","    # get the inputs\n","    print(\"Enumerate: Train loader loop\")\n","    inputs,labels=data\n","    # wrap them in variable\n","    inputs,labels=Variable(inputs).to(mydevice),Variable(labels).to(mydevice)\n","\n","    if not use_lbfgs:\n","     # zero gradients\n","     optimizer.zero_grad()\n","     # forward+backward optimize\n","     outputs=net(inputs)\n","     loss=criterion(outputs,labels)\n","     loss.backward()\n","     optimizer.step()\n","    else:\n","      if not wide_resnet:\n","        layer1=torch.cat([x.view(-1) for x in net.layer1.parameters()])\n","        layer2=torch.cat([x.view(-1) for x in net.layer2.parameters()])\n","        layer3=torch.cat([x.view(-1) for x in net.layer3.parameters()])\n","        layer4=torch.cat([x.view(-1) for x in net.layer4.parameters()])\n","\n","      def closure():\n","        if torch.is_grad_enabled():\n","         optimizer.zero_grad()   # Doing this\n","        outputs=net(inputs)\n","        if not wide_resnet:  # Doing this\n","          l1_penalty=lambda1*(torch.norm(layer1,1)+torch.norm(layer2,1)+torch.norm(layer3,1)+torch.norm(layer4,1))\n","          l2_penalty=lambda2*(torch.norm(layer1,2)+torch.norm(layer2,2)+torch.norm(layer3,2)+torch.norm(layer4,2))\n","          loss=criterion(outputs,labels)+l1_penalty+l2_penalty\n","        else:\n","          l1_penalty=0\n","          l2_penalty=0\n","          loss=criterion(outputs,labels)\n","        if loss.requires_grad:  # Doing this\n","          loss.backward()\n","          #print('loss %f l1 %f l2 %f'%(loss,l1_penalty,l2_penalty))\n","        return loss\n","      optimizer.step(closure)\n","    # only for diagnostics\n","    outputs=net(inputs)\n","    loss=criterion(outputs,labels)\n","    running_loss +=loss.data.item()\n","\n","    if math.isnan(loss.data.item()):\n","       print('loss became nan at %d'%i)\n","       break\n","\n","    # print statistics\n","    if i%(batches_for_report) == (batches_for_report-1): # after every 'batches_for_report'\n","      print('%f: [%d, %5d] loss: %.5f accuracy: %.3f'%\n","         (time.time()-start_time,epoch+1,i+1,running_loss/batches_for_report,\n","         verification_error_check(net)))\n","      running_loss=0.0\n","\n","print('Finished Training')\n","\n","\n","# save model (and other extra items)\n","torch.save({\n","            'model_state_dict':net.state_dict(),\n","            'epoch':epoch,\n","            'optimizer_state_dict':optimizer.state_dict(),\n","            'running_loss':running_loss,\n","           },'./res.model')\n","\n","# whole dataset\n","correct=0\n","total=0\n","for data in trainloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   print(\"Train loader loop\")\n","   \n","print('Accuracy of the network on the %d train images: %d %%'%\n","    (total,100*correct//total))\n","\n","correct=0\n","total=0\n","for data in testloader:\n","   images,labels=data\n","   outputs=net(Variable(images).to(mydevice)).cpu()\n","   _,predicted=torch.max(outputs.data,1)\n","   total += labels.size(0)\n","   correct += (predicted==labels).sum()\n","   \n","print('Accuracy of the network on the %d test images: %d %%'%\n","    (total,100*correct//total))\n","\n","\n","class_correct=list(0. for i in range(4))\n","class_total=list(0. for i in range(4))\n","for data in testloader:\n","  images,labels=data\n","  outputs=net(Variable(images).to(mydevice)).cpu()\n","  _,predicted=torch.max(outputs.data,1)\n","  c=(predicted==labels).squeeze()\n","  for i in range(4):\n","    label=labels[i]\n","    class_correct[label] += c[i]\n","    class_total[label] += 1\n","\n","for i in range(4):\n","  print('Accuracy of %5s : %2d %%' %\n","    (classes[i],100*float(class_correct[i])/float(class_total[i])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_9OVrQkju74O","executionInfo":{"status":"error","timestamp":1681337520670,"user_tz":360,"elapsed":5341,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}},"outputId":"b3c35402-2ebb-4d38-cf5d-72305839731c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA\n","<bound method Module.parameters of ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=4, bias=True)\n",")>\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Enumerate: Train loader loop\n","Finished Training\n","Train loader loop\n","Accuracy of the network on the 80 train images: 78 %\n","Accuracy of the network on the 44 test images: 27 %\n","Accuracy of n01-randomTgts :  0 %\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-29ea438454d1>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   print('Accuracy of %5s : %2d %%' %\n\u001b[0;32m--> 183\u001b[0;31m     (classes[i],100*float(class_correct[i])/float(class_total[i])))\n\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"]}]},{"cell_type":"markdown","source":["# **Full batch L-BFGS (simple equation)**\n","About 60% accuracy"],"metadata":{"id":"Kvxe7qwMhXAh"}},{"cell_type":"code","source":["\"\"\"\n","Full-Overlap L-BFGS Implementation with Stochastic Wolfe Line Search\n","\n","Demonstrates how to implement full-overlap L-BFGS with stochastic weak Wolfe line\n","search without Powell damping to train a simple convolutional neural network using the \n","LBFGS optimizer. Full-overlap L-BFGS is a stochastic quasi-Newton method that uses \n","the same sample as the one used in the stochastic gradient to perform quasi-Newton \n","updating, then resamples an entirely independent new sample in the next iteration.\n","\n","This implementation is CUDA-compatible.\n","\n","Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n","Last edited 10/20/20.\n","\n","Requirements:\n","    - Keras (for CIFAR-10 dataset)\n","    - NumPy\n","    - PyTorch\n","\n","Run Command:\n","    python full_overlap_lbfgs_example.py\n","\n","Based on stable quasi-Newton updating introduced by Schraudolph, Yu, and Gunter in\n","\"A Stochastic Quasi-Newton Method for Online Convex Optimization\" (2007)\n","\n","\"\"\"\n","\n","import sys\n","sys.path.append('../../functions/')\n","\n","import numpy as np\n","import torch\n","import torch.optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tensorflow.keras.datasets import cifar10 # to load dataset\n","\n","from utils import compute_stats, get_grad\n","from LBFGS import LBFGS\n","\n","# Parameters for L-BFGS training\n","max_iter = 200\n","ghost_batch = 128\n","batch_size = 8192\n","\n","# Load data\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train = X_train / 255\n","X_test = X_test / 255\n","\n","X_train = np.transpose(X_train, (0, 3, 1, 2))\n","X_test = np.transpose(X_test, (0, 3, 1, 2))\n","\n","# Define network\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 1000)\n","        self.fc2 = nn.Linear(1000, 10)\n","        \n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Check cuda availability\n","cuda = torch.cuda.is_available()\n","    \n","# Create neural network model\n","if cuda:\n","    torch.cuda.manual_seed(2018)\n","    model = ConvNet().cuda() \n","else:\n","    torch.manual_seed(2018)\n","    model = ConvNet()\n","    \n","# Define helper functions\n","\n","# Forward pass\n","if cuda:\n","    opfun = lambda X: model.forward(torch.from_numpy(X).cuda())\n","else:\n","    opfun = lambda X: model.forward(torch.from_numpy(X))\n","\n","# Forward pass through the network given the input\n","if cuda:\n","    predsfun = lambda op: np.argmax(op.cpu().data.numpy(), 1)\n","else:\n","    predsfun = lambda op: np.argmax(op.data.numpy(), 1)\n","\n","# Do the forward pass, then compute the accuracy\n","accfun = lambda op, y: np.mean(np.equal(predsfun(op), y.squeeze())) * 100\n","\n","# Define optimizer\n","optimizer = LBFGS(model.parameters(), lr=1., history_size=10, line_search='Wolfe', debug=True)\n","\n","# Main training loop\n","for n_iter in range(max_iter):\n","    \n","    # training mode\n","    model.train()\n","    \n","    # sample batch\n","    random_index = np.random.permutation(range(X_train.shape[0]))\n","    Sk = random_index[0:batch_size]\n","    \n","    # compute initial gradient and objective\n","    grad, obj = get_grad(optimizer, X_train[Sk], y_train[Sk], opfun)\n","    \n","    # two-loop recursion to compute search direction\n","    p = optimizer.two_loop_recursion(-grad)\n","            \n","    # define closure for line search\n","    def closure():              \n","        \n","        optimizer.zero_grad()\n","        \n","        if cuda:\n","            loss_fn = torch.tensor(0, dtype=torch.float).cuda()\n","        else:\n","            loss_fn = torch.tensor(0, dtype=torch.float)\n","        \n","        for subsmpl in np.array_split(Sk, max(int(batch_size / ghost_batch), 1)):\n","                        \n","            ops = opfun(X_train[subsmpl])\n","            \n","            if cuda:\n","                tgts = torch.from_numpy(y_train[subsmpl]).cuda().long().squeeze()\n","            else:\n","                tgts = torch.from_numpy(y_train[subsmpl]).long().squeeze()\n","                \n","            loss_fn += F.cross_entropy(ops, tgts) * (len(subsmpl) / batch_size)\n","                        \n","        return loss_fn\n","    \n","    # perform line search step\n","    options = {'closure': closure, 'current_loss': obj}\n","    obj, grad, lr, _, _, _, _, _ = optimizer.step(p, grad, options=options)\n","        \n","    # curvature update\n","    optimizer.curvature_update(grad)\n","    \n","    # compute statistics\n","    model.eval()\n","    train_loss, test_loss, test_acc = compute_stats(X_train, y_train, X_test, y_test, opfun, accfun,\n","                                                    ghost_batch=128)\n","            \n","    # print data\n","    print('Iter:', n_iter + 1, 'lr:', lr, 'Training Loss:', train_loss, 'Test Loss:', test_loss,\n","          'Test Accuracy:', test_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MW7GsqFShiDr","executionInfo":{"status":"ok","timestamp":1680901601200,"user_tz":360,"elapsed":183341,"user":{"displayName":"MyStale Doritos","userId":"09193328873768151182"}},"outputId":"e5c5782b-f2c9-4883-d2dc-9674f971b845"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 13s 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/Modules/LBFGS.py:257: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"]},{"output_type":"stream","name":"stdout","text":["Iter: 1 lr: 1.0 Training Loss: 2.3005604695272446 Test Loss: 2.300521916341781 Test Accuracy: 10.889999999999999\n","Iter: 2 lr: 1.0 Training Loss: 2.291775402274132 Test Loss: 2.2916575891733175 Test Accuracy: 10.28\n","Iter: 3 lr: 1.0 Training Loss: 2.2845198953819277 Test Loss: 2.2841987432479867 Test Accuracy: 11.980000000000002\n","Iter: 4 lr: 1.0 Training Loss: 2.269330383758546 Test Loss: 2.2691428013086323 Test Accuracy: 12.540000000000003\n","Iter: 5 lr: 1.0 Training Loss: 2.2602930858325956 Test Loss: 2.260551500034332 Test Accuracy: 15.54\n","Iter: 6 lr: 1.0 Training Loss: 2.2395996826696396 Test Loss: 2.2399813642263404 Test Accuracy: 14.600000000000005\n","Iter: 7 lr: 0.0956545493549986 Training Loss: 2.224844878101348 Test Loss: 2.2254613263368612 Test Accuracy: 14.360000000000008\n","Iter: 8 lr: 1.0 Training Loss: 2.1973313039636615 Test Loss: 2.198421651649474 Test Accuracy: 17.370000000000005\n","Iter: 9 lr: 0.06638844340475818 Training Loss: 2.167039254875183 Test Loss: 2.1692809090137484 Test Accuracy: 19.310000000000002\n","Iter: 10 lr: 1.0 Training Loss: 2.142632847342491 Test Loss: 2.144713118505478 Test Accuracy: 19.999999999999996\n","Iter: 11 lr: 1.0 Training Loss: 2.1260812445783612 Test Loss: 2.1276297363758085 Test Accuracy: 19.67\n","Iter: 12 lr: 1.0 Training Loss: 2.1195907910585396 Test Loss: 2.1205919328212732 Test Accuracy: 19.84\n","Iter: 13 lr: 1.0 Training Loss: 2.091233996767998 Test Loss: 2.0875299735784516 Test Accuracy: 21.559999999999995\n","Iter: 14 lr: 1.0 Training Loss: 2.0742609044551847 Test Loss: 2.0695039175033565 Test Accuracy: 21.889999999999997\n","Iter: 15 lr: 1.0 Training Loss: 2.072502172822952 Test Loss: 2.067122700595856 Test Accuracy: 21.97\n","Iter: 16 lr: 1.0 Training Loss: 2.071143428812028 Test Loss: 2.0663489974141114 Test Accuracy: 21.790000000000003\n","Iter: 17 lr: 1.0 Training Loss: 2.069205620522499 Test Loss: 2.064021888327599 Test Accuracy: 21.91000000000001\n","Iter: 18 lr: 1.0 Training Loss: 2.0644083966922766 Test Loss: 2.0589021088600163 Test Accuracy: 21.979999999999997\n","Iter: 19 lr: 0.32140300832386864 Training Loss: 2.0494279478788373 Test Loss: 2.043021292662621 Test Accuracy: 23.06\n","Iter: 20 lr: 0.2 Training Loss: 2.043127831981182 Test Loss: 2.0357744533300406 Test Accuracy: 24.530000000000005\n","Iter: 21 lr: 1.0 Training Loss: 2.0374997210454944 Test Loss: 2.029172400522231 Test Accuracy: 24.50999999999999\n","Iter: 22 lr: 1.0 Training Loss: 2.0216525054073333 Test Loss: 2.0121322785139086 Test Accuracy: 25.160000000000004\n","Iter: 23 lr: 1.0 Training Loss: 2.004714668571949 Test Loss: 1.9950961193799979 Test Accuracy: 25.960000000000004\n","Iter: 24 lr: 1.0 Training Loss: 2.0027848524832725 Test Loss: 1.992521639084816 Test Accuracy: 26.649999999999984\n","Iter: 25 lr: 1.0 Training Loss: 1.9896114313316347 Test Loss: 1.9821286657929422 Test Accuracy: 26.62\n","Iter: 26 lr: 1.0 Training Loss: 1.9839761631178854 Test Loss: 1.975377978801727 Test Accuracy: 27.009999999999994\n","Iter: 27 lr: 1.0 Training Loss: 1.9755924173903463 Test Loss: 1.9661814056038858 Test Accuracy: 27.459999999999994\n","Iter: 28 lr: 1.0 Training Loss: 1.9661610833263392 Test Loss: 1.9573999094605445 Test Accuracy: 28.069999999999997\n","Iter: 29 lr: 1.0 Training Loss: 1.9557806831121443 Test Loss: 1.9489699578881257 Test Accuracy: 28.65000000000001\n","Iter: 30 lr: 1.0 Training Loss: 1.9471100136256225 Test Loss: 1.9415685842037198 Test Accuracy: 28.599999999999987\n","Iter: 31 lr: 1.0 Training Loss: 1.9306879073190693 Test Loss: 1.9272321732401854 Test Accuracy: 28.700000000000003\n","Iter: 32 lr: 1.0 Training Loss: 1.9208490864539147 Test Loss: 1.9214862118363385 Test Accuracy: 28.92\n","Iter: 33 lr: 1.0 Training Loss: 1.9142214837861058 Test Loss: 1.9141184397935866 Test Accuracy: 29.319999999999993\n","Iter: 34 lr: 1.0 Training Loss: 1.909377014839649 Test Loss: 1.906762621688842 Test Accuracy: 29.750000000000007\n","Iter: 35 lr: 1.0 Training Loss: 1.905936950967312 Test Loss: 1.9044316752195358 Test Accuracy: 29.760000000000005\n","Iter: 36 lr: 1.0 Training Loss: 1.901889744820595 Test Loss: 1.900956972050666 Test Accuracy: 29.879999999999985\n","Iter: 37 lr: 1.0 Training Loss: 1.89546034015894 Test Loss: 1.894266868698598 Test Accuracy: 30.319999999999997\n","Iter: 38 lr: 0.4971946631205455 Training Loss: 1.8887512066054337 Test Loss: 1.8851982507824891 Test Accuracy: 30.14000000000001\n","Iter: 39 lr: 1.0 Training Loss: 1.8794125881242743 Test Loss: 1.8757212826967238 Test Accuracy: 30.640000000000004\n","Iter: 40 lr: 1.0 Training Loss: 1.8753829199743273 Test Loss: 1.8721792713761334 Test Accuracy: 30.710000000000015\n","Iter: 41 lr: 1.0 Training Loss: 1.874079500153065 Test Loss: 1.8711245400071146 Test Accuracy: 30.529999999999994\n","Iter: 42 lr: 1.0 Training Loss: 1.8711367480158803 Test Loss: 1.8680290259718897 Test Accuracy: 30.76\n","Iter: 43 lr: 1.0 Training Loss: 1.8599196665167814 Test Loss: 1.8565926240324981 Test Accuracy: 30.900000000000006\n","Iter: 44 lr: 1.0 Training Loss: 1.8486309711599354 Test Loss: 1.8458822080016137 Test Accuracy: 31.080000000000002\n","Iter: 45 lr: 1.0 Training Loss: 1.8442891019368168 Test Loss: 1.84250798689127 Test Accuracy: 31.519999999999996\n","Iter: 46 lr: 1.0 Training Loss: 1.8390320933604234 Test Loss: 1.8360290869593625 Test Accuracy: 31.780000000000005\n","Iter: 47 lr: 1.0 Training Loss: 1.8227300430631646 Test Loss: 1.8201503571271898 Test Accuracy: 32.68\n","Iter: 48 lr: 1.0 Training Loss: 1.8170155347490309 Test Loss: 1.812344781577587 Test Accuracy: 33.330000000000005\n","Iter: 49 lr: 1.0 Training Loss: 1.8088302369403841 Test Loss: 1.8045546245574957 Test Accuracy: 33.55000000000001\n","Iter: 50 lr: 1.0 Training Loss: 1.8068245593428611 Test Loss: 1.803366778635979 Test Accuracy: 33.82000000000001\n","Iter: 51 lr: 0.44124189656935703 Training Loss: 1.8056613043546674 Test Loss: 1.8024270469546315 Test Accuracy: 33.51\n","Iter: 52 lr: 1.0 Training Loss: 1.801661218848228 Test Loss: 1.7976851401686667 Test Accuracy: 33.81\n","Iter: 53 lr: 1.0 Training Loss: 1.7977078997039793 Test Loss: 1.793442539429664 Test Accuracy: 34.11000000000001\n","Iter: 54 lr: 1.0 Training Loss: 1.7885411477541926 Test Loss: 1.7844166063308722 Test Accuracy: 34.10000000000001\n","Iter: 55 lr: 1.0 Training Loss: 1.7810931324982635 Test Loss: 1.7772284617304803 Test Accuracy: 34.19\n","Iter: 56 lr: 1.0 Training Loss: 1.7757854939246178 Test Loss: 1.7719523131370547 Test Accuracy: 34.42\n","Iter: 57 lr: 1.0 Training Loss: 1.7694516790556907 Test Loss: 1.765498132455349 Test Accuracy: 34.620000000000005\n","Iter: 58 lr: 0.2 Training Loss: 1.768272367734909 Test Loss: 1.7632202002525332 Test Accuracy: 34.57\n","Iter: 59 lr: 1.0 Training Loss: 1.762533257620335 Test Loss: 1.7577730037093164 Test Accuracy: 34.74999999999999\n","Iter: 60 lr: 1.0 Training Loss: 1.7601863777828215 Test Loss: 1.755288926589489 Test Accuracy: 35.070000000000014\n","Iter: 61 lr: 1.0 Training Loss: 1.7558638158154494 Test Loss: 1.751048126292229 Test Accuracy: 35.63\n","Iter: 62 lr: 1.0 Training Loss: 1.749447422735691 Test Loss: 1.7441369691848754 Test Accuracy: 36.17000000000001\n","Iter: 63 lr: 1.0 Training Loss: 1.7413902924656863 Test Loss: 1.7347582740902892 Test Accuracy: 37.08\n","Iter: 64 lr: 1.0 Training Loss: 1.7360832790637022 Test Loss: 1.728608934938907 Test Accuracy: 37.4\n","Iter: 65 lr: 1.0 Training Loss: 1.7337797208571433 Test Loss: 1.7263526472568511 Test Accuracy: 37.44\n","Iter: 66 lr: 1.0 Training Loss: 1.7306960338902468 Test Loss: 1.7231427104473123 Test Accuracy: 37.36\n","Iter: 67 lr: 1.0 Training Loss: 1.7289239632391924 Test Loss: 1.7210316075801846 Test Accuracy: 37.370000000000005\n","Iter: 68 lr: 1.0 Training Loss: 1.7250039621162412 Test Loss: 1.7165925510883333 Test Accuracy: 37.529999999999994\n","Iter: 69 lr: 1.0 Training Loss: 1.7154918919444084 Test Loss: 1.7059360511064534 Test Accuracy: 38.10999999999999\n","Iter: 70 lr: 1.0 Training Loss: 1.710353275630474 Test Loss: 1.7004190906167023 Test Accuracy: 38.20999999999998\n","Iter: 71 lr: 1.0 Training Loss: 1.7036014919376372 Test Loss: 1.6947861309528351 Test Accuracy: 38.39999999999999\n","Iter: 72 lr: 1.0 Training Loss: 1.6933313787913324 Test Loss: 1.6846958998680113 Test Accuracy: 38.89000000000002\n","Iter: 73 lr: 1.0 Training Loss: 1.6845109127044675 Test Loss: 1.6773053095340729 Test Accuracy: 39.37000000000001\n","Iter: 74 lr: 1.0 Training Loss: 1.6807676209568976 Test Loss: 1.6748805272221563 Test Accuracy: 39.480000000000004\n","Iter: 75 lr: 1.0 Training Loss: 1.6793813275551794 Test Loss: 1.6731588815450662 Test Accuracy: 39.739999999999995\n","Iter: 76 lr: 1.0 Training Loss: 1.6755200774669647 Test Loss: 1.6694885058641438 Test Accuracy: 39.670000000000016\n","Iter: 77 lr: 1.0 Training Loss: 1.6724565807247154 Test Loss: 1.6671701881885526 Test Accuracy: 39.849999999999994\n","Iter: 78 lr: 1.0 Training Loss: 1.6664568296170232 Test Loss: 1.6622582500100134 Test Accuracy: 40.27\n","Iter: 79 lr: 1.0 Training Loss: 1.664644416234493 Test Loss: 1.661576657295227 Test Accuracy: 40.520000000000024\n","Iter: 80 lr: 1.0 Training Loss: 1.6636079346895218 Test Loss: 1.660511726915837 Test Accuracy: 40.44999999999998\n","Iter: 81 lr: 1.0 Training Loss: 1.6612977817583083 Test Loss: 1.6578343283176429 Test Accuracy: 40.589999999999996\n","Iter: 82 lr: 1.0 Training Loss: 1.6570680469942094 Test Loss: 1.6535186413884162 Test Accuracy: 40.51\n","Iter: 83 lr: 1.0 Training Loss: 1.6579887722539897 Test Loss: 1.6569023449063296 Test Accuracy: 39.61000000000001\n","Iter: 84 lr: 1.0 Training Loss: 1.6508541471767424 Test Loss: 1.6489556097865108 Test Accuracy: 40.17999999999998\n","Iter: 85 lr: 1.0 Training Loss: 1.6497619959259024 Test Loss: 1.6484746883749957 Test Accuracy: 40.14000000000001\n","Iter: 86 lr: 0.36319422141373503 Training Loss: 1.6492031710147854 Test Loss: 1.6482581067204474 Test Accuracy: 40.170000000000016\n","Iter: 87 lr: 1.0 Training Loss: 1.6475212761378293 Test Loss: 1.6465540533900265 Test Accuracy: 40.35999999999997\n","Iter: 88 lr: 1.0 Training Loss: 1.6395133365035057 Test Loss: 1.638756670928001 Test Accuracy: 40.38999999999999\n","Iter: 89 lr: 1.0 Training Loss: 1.6343074667024609 Test Loss: 1.633316892004013 Test Accuracy: 40.76\n","Iter: 90 lr: 1.0 Training Loss: 1.6287032145071032 Test Loss: 1.6274135040879247 Test Accuracy: 40.90000000000001\n","Iter: 91 lr: 1.0 Training Loss: 1.626242148153782 Test Loss: 1.624761255061626 Test Accuracy: 40.880000000000024\n","Iter: 92 lr: 1.0 Training Loss: 1.6212383179259293 Test Loss: 1.6200821528077123 Test Accuracy: 41.21000000000001\n","Iter: 93 lr: 1.0 Training Loss: 1.6171668828868868 Test Loss: 1.6161808836340905 Test Accuracy: 41.29\n","Iter: 94 lr: 1.0 Training Loss: 1.612093612720966 Test Loss: 1.6121851480364802 Test Accuracy: 41.580000000000005\n","Iter: 95 lr: 1.0 Training Loss: 1.6066747035980222 Test Loss: 1.608121146512032 Test Accuracy: 41.45000000000003\n","Iter: 96 lr: 1.0 Training Loss: 1.6051293230032921 Test Loss: 1.606685016584396 Test Accuracy: 41.800000000000004\n","Iter: 97 lr: 1.0 Training Loss: 1.5970669750094415 Test Loss: 1.599451908922196 Test Accuracy: 42.230000000000004\n","Iter: 98 lr: 0.4939840681988529 Training Loss: 1.5942964359474183 Test Loss: 1.5965551793217665 Test Accuracy: 42.1\n","Iter: 99 lr: 1.0 Training Loss: 1.5930575574946404 Test Loss: 1.5956439339399338 Test Accuracy: 42.27000000000001\n","Iter: 100 lr: 1.0 Training Loss: 1.5900291073989874 Test Loss: 1.5931391345620167 Test Accuracy: 42.41000000000001\n","Iter: 101 lr: 1.0 Training Loss: 1.58814209606886 Test Loss: 1.5933634001612658 Test Accuracy: 42.359999999999985\n","Iter: 102 lr: 1.0 Training Loss: 1.5833137846779826 Test Loss: 1.5888331434011458 Test Accuracy: 42.48999999999999\n","Iter: 103 lr: 1.0 Training Loss: 1.5803280520892138 Test Loss: 1.5864835604190828 Test Accuracy: 42.61000000000003\n","Iter: 104 lr: 1.0 Training Loss: 1.5785923584365846 Test Loss: 1.5848286674022676 Test Accuracy: 42.78\n","Iter: 105 lr: 1.0 Training Loss: 1.574654484653473 Test Loss: 1.5824285924673078 Test Accuracy: 43.11\n","Iter: 106 lr: 1.0 Training Loss: 1.5706224769663808 Test Loss: 1.5784913658142092 Test Accuracy: 43.00999999999999\n","Iter: 107 lr: 1.0 Training Loss: 1.5677997121620177 Test Loss: 1.5765242032289508 Test Accuracy: 43.13000000000001\n","Iter: 108 lr: 1.0 Training Loss: 1.5686842671275143 Test Loss: 1.5757095089316366 Test Accuracy: 43.04999999999999\n","Iter: 109 lr: 1.0 Training Loss: 1.563780991165638 Test Loss: 1.5715212096333502 Test Accuracy: 43.23000000000001\n","Iter: 110 lr: 1.0 Training Loss: 1.5624556606483462 Test Loss: 1.5703608136773106 Test Accuracy: 43.15\n","Iter: 111 lr: 0.340601448851632 Training Loss: 1.5612599509239196 Test Loss: 1.5702539359688756 Test Accuracy: 43.52999999999999\n","Iter: 112 lr: 1.0 Training Loss: 1.5600838569760322 Test Loss: 1.5683519216060637 Test Accuracy: 43.37000000000001\n","Iter: 113 lr: 1.0 Training Loss: 1.5580046027207377 Test Loss: 1.5661486926436425 Test Accuracy: 43.62\n","Iter: 114 lr: 1.0 Training Loss: 1.554946823554039 Test Loss: 1.5635744183897975 Test Accuracy: 43.730000000000025\n","Iter: 115 lr: 1.0 Training Loss: 1.551124765250683 Test Loss: 1.558471200311184 Test Accuracy: 43.91000000000001\n","Iter: 116 lr: 1.0 Training Loss: 1.5497538804697988 Test Loss: 1.5574164030432702 Test Accuracy: 44.13000000000001\n","Iter: 117 lr: 1.0 Training Loss: 1.5499860081839563 Test Loss: 1.55898366445303 Test Accuracy: 43.46\n","Iter: 118 lr: 1.0 Training Loss: 1.542970404727459 Test Loss: 1.5508180832624436 Test Accuracy: 43.83\n","Iter: 119 lr: 1.0 Training Loss: 1.5404095350456235 Test Loss: 1.548606845474243 Test Accuracy: 44.16000000000001\n","Iter: 120 lr: 1.0 Training Loss: 1.53889863437891 Test Loss: 1.5473668275713925 Test Accuracy: 44.43000000000001\n","Iter: 121 lr: 1.0 Training Loss: 1.536750182213783 Test Loss: 1.5448956037044528 Test Accuracy: 44.42000000000001\n","Iter: 122 lr: 1.0 Training Loss: 1.5342780885362624 Test Loss: 1.5421398890137674 Test Accuracy: 44.51\n","Iter: 123 lr: 1.0 Training Loss: 1.5355522916460034 Test Loss: 1.541207196211815 Test Accuracy: 44.499999999999986\n","Iter: 124 lr: 1.0 Training Loss: 1.5310385752105713 Test Loss: 1.538273857426643 Test Accuracy: 44.53999999999999\n","Iter: 125 lr: 1.0 Training Loss: 1.530008090701103 Test Loss: 1.5376415700435644 Test Accuracy: 44.44000000000002\n","Iter: 126 lr: 1.0 Training Loss: 1.527979585084915 Test Loss: 1.535878409099579 Test Accuracy: 44.42\n","Iter: 127 lr: 1.0 Training Loss: 1.5246350473570818 Test Loss: 1.5332973485469816 Test Accuracy: 44.58000000000001\n","Iter: 128 lr: 1.0 Training Loss: 1.5216379756808283 Test Loss: 1.531428546631336 Test Accuracy: 44.82\n","Iter: 129 lr: 0.4388077036420684 Training Loss: 1.5196953651309013 Test Loss: 1.530095293736458 Test Accuracy: 44.730000000000004\n","Iter: 130 lr: 1.0 Training Loss: 1.517663589699268 Test Loss: 1.5274534626364717 Test Accuracy: 44.830000000000005\n","Iter: 131 lr: 1.0 Training Loss: 1.5168895148324968 Test Loss: 1.5265860289216042 Test Accuracy: 44.779999999999994\n","Iter: 132 lr: 1.0 Training Loss: 1.5153282269024846 Test Loss: 1.5251716693520547 Test Accuracy: 44.86000000000001\n","Iter: 133 lr: 1.0 Training Loss: 1.5128785730957983 Test Loss: 1.5227735250592227 Test Accuracy: 44.97999999999998\n","Iter: 134 lr: 1.0 Training Loss: 1.5129167613577843 Test Loss: 1.5233902458190918 Test Accuracy: 44.59\n","Iter: 135 lr: 1.0 Training Loss: 1.510046498219967 Test Loss: 1.5205752403616906 Test Accuracy: 45.11000000000003\n","Iter: 136 lr: 1.0 Training Loss: 1.5075800002479554 Test Loss: 1.5179489139676097 Test Accuracy: 45.14000000000001\n","Iter: 137 lr: 0.3496832379955028 Training Loss: 1.5070728033232685 Test Loss: 1.5177475572943686 Test Accuracy: 44.98000000000002\n","Iter: 138 lr: 1.0 Training Loss: 1.5054955708742146 Test Loss: 1.5162852537393574 Test Accuracy: 45.18\n","Iter: 139 lr: 1.0 Training Loss: 1.5038874678277967 Test Loss: 1.5149137953042988 Test Accuracy: 45.35\n","Iter: 140 lr: 1.0 Training Loss: 1.501730844364166 Test Loss: 1.5133671861767775 Test Accuracy: 45.300000000000004\n","Iter: 141 lr: 1.0 Training Loss: 1.4997002882623673 Test Loss: 1.5116765755891794 Test Accuracy: 45.349999999999994\n","Iter: 142 lr: 1.0 Training Loss: 1.4989294771957393 Test Loss: 1.511666922044754 Test Accuracy: 45.53\n","Iter: 143 lr: 1.0 Training Loss: 1.4962355545091626 Test Loss: 1.5087233518004421 Test Accuracy: 45.51999999999998\n","Iter: 144 lr: 1.0 Training Loss: 1.4940836784696574 Test Loss: 1.5061836089611058 Test Accuracy: 45.59999999999999\n","Iter: 145 lr: 0.20540657876384455 Training Loss: 1.4916316254234314 Test Loss: 1.504434217882157 Test Accuracy: 45.81999999999999\n","Iter: 146 lr: 1.0 Training Loss: 1.4884080241394047 Test Loss: 1.5009008192539215 Test Accuracy: 45.84\n","Iter: 147 lr: 1.0 Training Loss: 1.484324022884369 Test Loss: 1.4975856011390687 Test Accuracy: 45.89999999999999\n","Iter: 148 lr: 1.0 Training Loss: 1.4816633151102072 Test Loss: 1.4954898839950561 Test Accuracy: 45.75000000000001\n","Iter: 149 lr: 1.0 Training Loss: 1.4796255483675005 Test Loss: 1.4937812766909597 Test Accuracy: 45.78999999999999\n","Iter: 150 lr: 1.0 Training Loss: 1.474960482141971 Test Loss: 1.4901269959807395 Test Accuracy: 45.980000000000004\n","Iter: 151 lr: 1.0 Training Loss: 1.4723824201393128 Test Loss: 1.4878896303892135 Test Accuracy: 46.24\n","Iter: 152 lr: 1.0 Training Loss: 1.4707784522438052 Test Loss: 1.4868685341715815 Test Accuracy: 45.760000000000005\n","Iter: 153 lr: 1.0 Training Loss: 1.469325865416527 Test Loss: 1.4852458799958224 Test Accuracy: 45.850000000000016\n","Iter: 154 lr: 1.0 Training Loss: 1.4676115303683277 Test Loss: 1.4829667133808138 Test Accuracy: 46.18000000000002\n","Iter: 155 lr: 1.0 Training Loss: 1.46533781914711 Test Loss: 1.4803098768949507 Test Accuracy: 46.20999999999999\n","Iter: 156 lr: 1.0 Training Loss: 1.4633542276215563 Test Loss: 1.478279741334915 Test Accuracy: 46.48\n","Iter: 157 lr: 1.0 Training Loss: 1.4623773957419397 Test Loss: 1.4786511549830441 Test Accuracy: 46.33\n","Iter: 158 lr: 1.0 Training Loss: 1.4615822248482706 Test Loss: 1.477531493091583 Test Accuracy: 46.299999999999976\n","Iter: 159 lr: 1.0 Training Loss: 1.4586690858840938 Test Loss: 1.4752156176686284 Test Accuracy: 46.55\n","Iter: 160 lr: 1.0 Training Loss: 1.4571150689268118 Test Loss: 1.4738028290510181 Test Accuracy: 46.62\n","Iter: 161 lr: 1.0 Training Loss: 1.455792451093197 Test Loss: 1.4723943832397457 Test Accuracy: 46.51\n","Iter: 162 lr: 1.0 Training Loss: 1.453672353174687 Test Loss: 1.470122286736965 Test Accuracy: 46.940000000000005\n","Iter: 163 lr: 1.0 Training Loss: 1.4509038925576212 Test Loss: 1.4679332286000253 Test Accuracy: 47.07000000000001\n","Iter: 164 lr: 1.0 Training Loss: 1.449816933305264 Test Loss: 1.466926947236061 Test Accuracy: 46.99000000000002\n","Iter: 165 lr: 1.0 Training Loss: 1.449320830118656 Test Loss: 1.4665524422049518 Test Accuracy: 46.88000000000001\n","Iter: 166 lr: 1.0 Training Loss: 1.4488475252771376 Test Loss: 1.4655439806222912 Test Accuracy: 47.13000000000001\n","Iter: 167 lr: 1.0 Training Loss: 1.445688339064121 Test Loss: 1.462489966571331 Test Accuracy: 47.18000000000001\n","Iter: 168 lr: 1.0 Training Loss: 1.4424567773342134 Test Loss: 1.4594374146223068 Test Accuracy: 47.179999999999986\n","Iter: 169 lr: 1.0 Training Loss: 1.4378960272526742 Test Loss: 1.4549016257286076 Test Accuracy: 47.469999999999985\n","Iter: 170 lr: 1.0 Training Loss: 1.4340892995285983 Test Loss: 1.4507921259284027 Test Accuracy: 47.64\n","Iter: 171 lr: 1.0 Training Loss: 1.43233893304348 Test Loss: 1.4490887066841125 Test Accuracy: 47.620000000000005\n","Iter: 172 lr: 1.0 Training Loss: 1.4312812297511102 Test Loss: 1.4483603809714314 Test Accuracy: 47.49999999999999\n","Iter: 173 lr: 1.0 Training Loss: 1.4304959198975555 Test Loss: 1.4480062052726745 Test Accuracy: 47.54\n","Iter: 174 lr: 1.0 Training Loss: 1.428964988644123 Test Loss: 1.4467831609606745 Test Accuracy: 47.57999999999999\n","Iter: 175 lr: 1.0 Training Loss: 1.4266501226830481 Test Loss: 1.4444769611477848 Test Accuracy: 47.78\n","Iter: 176 lr: 1.0 Training Loss: 1.4246583161044124 Test Loss: 1.4435104148387907 Test Accuracy: 47.99999999999999\n","Iter: 177 lr: 1.0 Training Loss: 1.4226283602190015 Test Loss: 1.4419204611182213 Test Accuracy: 47.88000000000003\n","Iter: 178 lr: 1.0 Training Loss: 1.4217057641959188 Test Loss: 1.4406399333238602 Test Accuracy: 48.01999999999999\n","Iter: 179 lr: 1.0 Training Loss: 1.419821721987724 Test Loss: 1.439284075844288 Test Accuracy: 48.100000000000016\n","Iter: 180 lr: 1.0 Training Loss: 1.4176764072346684 Test Loss: 1.4376156397461892 Test Accuracy: 47.98\n","Iter: 181 lr: 1.0 Training Loss: 1.4198201402354234 Test Loss: 1.4417924408078193 Test Accuracy: 47.689999999999976\n","Iter: 182 lr: 1.0 Training Loss: 1.4155093387269977 Test Loss: 1.437271156847477 Test Accuracy: 48.08999999999998\n","Iter: 183 lr: 1.0 Training Loss: 1.4144579941511148 Test Loss: 1.4355747363924987 Test Accuracy: 48.25\n","Iter: 184 lr: 1.0 Training Loss: 1.4149548670363428 Test Loss: 1.4367917771458636 Test Accuracy: 48.39999999999998\n","Iter: 185 lr: 1.0 Training Loss: 1.41527950601101 Test Loss: 1.4366878624677655 Test Accuracy: 48.17\n","Iter: 186 lr: 1.0 Training Loss: 1.4113715910005569 Test Loss: 1.4328910981178276 Test Accuracy: 48.399999999999984\n","Iter: 187 lr: 1.0 Training Loss: 1.4107765485095982 Test Loss: 1.433031025516986 Test Accuracy: 48.51000000000002\n","Iter: 188 lr: 0.44660783062007353 Training Loss: 1.4101822641181947 Test Loss: 1.4322813645243644 Test Accuracy: 48.55000000000001\n","Iter: 189 lr: 1.0 Training Loss: 1.408262769958973 Test Loss: 1.4303067063331603 Test Accuracy: 48.48000000000001\n","Iter: 190 lr: 1.0 Training Loss: 1.4049386569762226 Test Loss: 1.4273288944244387 Test Accuracy: 48.66\n","Iter: 191 lr: 1.0 Training Loss: 1.4034723141026502 Test Loss: 1.4264848209619525 Test Accuracy: 48.920000000000016\n","Iter: 192 lr: 1.0 Training Loss: 1.4026795515251163 Test Loss: 1.426344256639481 Test Accuracy: 48.95000000000002\n","Iter: 193 lr: 1.0 Training Loss: 1.401944371130467 Test Loss: 1.4256919625401496 Test Accuracy: 48.99\n","Iter: 194 lr: 1.0 Training Loss: 1.4009633224320404 Test Loss: 1.4249002390503884 Test Accuracy: 49.12999999999999\n","Iter: 195 lr: 1.0 Training Loss: 1.400376382339001 Test Loss: 1.425236415195465 Test Accuracy: 49.06000000000001\n","Iter: 196 lr: 1.0 Training Loss: 1.3987600685453416 Test Loss: 1.4233823214888572 Test Accuracy: 49.170000000000016\n","Iter: 197 lr: 1.0 Training Loss: 1.3970866664075854 Test Loss: 1.4213370798826215 Test Accuracy: 49.28000000000001\n","Iter: 198 lr: 1.0 Training Loss: 1.3940523945260053 Test Loss: 1.4184633518099787 Test Accuracy: 49.110000000000014\n","Iter: 199 lr: 0.4877341520548353 Training Loss: 1.393728331563472 Test Loss: 1.4175932487249374 Test Accuracy: 48.92\n","Iter: 200 lr: 1.0 Training Loss: 1.3923773256373404 Test Loss: 1.4166968775153164 Test Accuracy: 49.19000000000001\n"]}]}]}